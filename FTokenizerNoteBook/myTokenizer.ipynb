{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\r\n",
    "import numpy as np\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open(\"..data/bi_grams.txt\",encoding=\"utf-8\") as f:\r\n",
    "    bi_gramtxt=f.read()\r\n",
    "bi_gram_list=[]\r\n",
    "for each in bi_gramtxt.split(\",\"):\r\n",
    "    each=each[:-1]\r\n",
    "    each=each[2:]\r\n",
    "    each=each.replace(\" \",\"_\")\r\n",
    "    bi_gram_list.append(each)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class FTokenizer():\r\n",
    "    def __init__(self,bi_gram_list):\r\n",
    "        self.bi_gram_list=bi_gram_list\r\n",
    "\r\n",
    "    def bi_gram_checker(self,bi_gram_word):\r\n",
    "        for i in self.bi_gram_list:\r\n",
    "            if (bi_gram_word==i):\r\n",
    "                return True\r\n",
    "        return False\r\n",
    "    \r\n",
    "    def fit_on_text(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (myContinue):\r\n",
    "                    myContinue=False\r\n",
    "                    continue\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        myContinue=True\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (myContinue==False):  \r\n",
    "                    if (tokens.count(listtokens[i])==0):\r\n",
    "                        tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "\r\n",
    "    def fit_on_text2(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (tokens.count(listtokens[i])==0 and listtokens[i]!=\"\"):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "    \r\n",
    "    def get_word_to_index(self):\r\n",
    "        return self.word_to_index\r\n",
    "    \r\n",
    "    def get_index_to_word(self):\r\n",
    "        return self.index_to_word\r\n",
    "\r\n",
    "    def get_max_length(self,data):\r\n",
    "        maxsen=0\r\n",
    "        for i in data:\r\n",
    "            if maxsen<len(i):\r\n",
    "                maxsen=len(i)\r\n",
    "        return maxsen\r\n",
    "\r\n",
    "    def text_to_sequence(self,textdata):\r\n",
    "        d_continue=False\r\n",
    "        output=[]\r\n",
    "        for text in textdata:\r\n",
    "            text=text.lower().split(\" \")\r\n",
    "            text.append(\"\")\r\n",
    "            sth=[]\r\n",
    "            d_continue=False\r\n",
    "            for i in range(0,len(text)-1):\r\n",
    "                if d_continue==True:\r\n",
    "                    d_continue=False\r\n",
    "                    continue\r\n",
    "                if (text[i+1]!=\"\"):\r\n",
    "                    pairtext=text[i]+\"_\"+text[i+1]\r\n",
    "                    for j in self.word_to_index:\r\n",
    "                        if pairtext.lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            d_continue=True\r\n",
    "                            break\r\n",
    "                if d_continue==False:\r\n",
    "                    for count,j in enumerate(self.word_to_index):\r\n",
    "                        if text[i].lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            break\r\n",
    "                        if count==len(self.word_to_index)-1:\r\n",
    "                            sth.append(1)\r\n",
    "            output.append(sth)\r\n",
    "        return output\r\n",
    "    \r\n",
    "    def intseq_to_index(self,sequence):\r\n",
    "        output=[]\r\n",
    "        for i in sequence:\r\n",
    "            output.append(self.index_to_word[i])\r\n",
    "        return output\r\n",
    "\r\n",
    "    def pad_sequence(self,datasequence,maxlen):\r\n",
    "        for i in datasequence:\r\n",
    "            while len(i)!=maxlen:\r\n",
    "                i.insert(0,0)\r\n",
    "        return datasequence\r\n",
    "\r\n",
    "    def bi_gram_ize(self,text):\r\n",
    "        listtokens=text.lower().split(\" \")\r\n",
    "        listtokens.append(\"\")\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for i in range(0,len(listtokens)-1):\r\n",
    "            if (myContinue):\r\n",
    "                myContinue=False\r\n",
    "                continue\r\n",
    "            if (listtokens[i+1]!=\"\"):\r\n",
    "                pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                if (self.bi_gram_checker(pairtext)):\r\n",
    "                    myContinue=True\r\n",
    "                    if (tokens.count(pairtext)==0):\r\n",
    "                        tokens.append(pairtext)\r\n",
    "            if (myContinue==False):  \r\n",
    "                if (tokens.count(listtokens[i])==0):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "        return tokens\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "Tokenizer=FTokenizer(bi_gram_list=bi_gram_list)\r\n",
    "Tokenizer.fit_on_text2([\"Hôm nay tôi đi học\",\"Hôm nay tôi đi chơi\",\"Bạn tới chơi nhà\",\"Có sao không vậy\",\"Tôi thích ăn sandwich\",\"Chơi minecraft đi\",\"Hôm nay tôi chưa làm xong bài tập quan trọng nữa\"])\r\n",
    "te=Tokenizer.text_to_sequence([\"Hôm nay tôi đi học\",\"Hôm nay tôi đi chơi\",\"Bạn tới chơi nhà\",\"Có sao không vậy\",\"Tôi thích ăn sandwich\",\"Chơi minecraft đi\",\"Hôm nay tôi chưa làm xong bài tập quan trọng nữa\"])\r\n",
    "maxlen=Tokenizer.get_max_length(te)\r\n",
    "output=Tokenizer.pad_sequence(te,maxlen)\r\n",
    "Tokenizer.word_to_index\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<OOV>': 1,\n",
       " 'hôm_nay': 2,\n",
       " 'hôm': 3,\n",
       " 'nay': 4,\n",
       " 'tôi': 5,\n",
       " 'đi': 6,\n",
       " 'học': 7,\n",
       " 'chơi': 8,\n",
       " 'bạn': 9,\n",
       " 'tới': 10,\n",
       " 'nhà': 11,\n",
       " 'có': 12,\n",
       " 'sao': 13,\n",
       " 'không': 14,\n",
       " 'vậy': 15,\n",
       " 'thích': 16,\n",
       " 'ăn': 17,\n",
       " 'sandwich': 18,\n",
       " 'minecraft': 19,\n",
       " 'chưa': 20,\n",
       " 'làm': 21,\n",
       " 'xong': 22,\n",
       " 'bài_tập': 23,\n",
       " 'bài': 24,\n",
       " 'tập': 25,\n",
       " 'quan_trọng': 26,\n",
       " 'quan': 27,\n",
       " 'trọng': 28,\n",
       " 'nữa': 29}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "print(Tokenizer.bi_gram_ize(\"Buổi họp hôm nay rất quan trọng\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['buổi', 'họp', 'hôm_nay', 'rất', 'quan_trọng']\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "3d95097792c4c1f8621d6eabaad3727eb6f403675c9e5393220cb43082419a68"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}