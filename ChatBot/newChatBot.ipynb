{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow_text as tf_text\n",
    "import FTokenizer\n",
    "import random\n",
    "import wikipedia as wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data Intent,Bi_gram,VN Stop word,posData\n",
    "\n",
    "with open(\"C:/Projects/personalProjects/pythonStuff/foxPython/ChatBot/intentsVNver1.json\",encoding=\"utf-8\") as file:\n",
    "    data=json.load(file)\n",
    "patternlist=[]\n",
    "labels=[]\n",
    "for dt in data[\"intents\"]:\n",
    "    for pattern in dt[\"patterns\"]:\n",
    "        patternlist.append(pattern)\n",
    "        labels.append(dt[\"tag\"])\n",
    "labels\n",
    "labels=np.array(labels)\n",
    "labelsOH=pd.get_dummies(labels)\n",
    "labellist=list(labelsOH)\n",
    "\n",
    "with open(\"C:/Projects/personalProjects/pythonStuff/foxPython/data/bi_grams.txt\",encoding=\"utf-8\") as f:\n",
    "    bi_gramtxt=f.read()\n",
    "bi_gram_list=[]\n",
    "for each in bi_gramtxt.split(\",\"):\n",
    "    each=each[:-1]\n",
    "    each=each[2:]\n",
    "    each=each.replace(\" \",\"_\")\n",
    "    bi_gram_list.append(each)\n",
    "\n",
    "with open(\"C:/Projects/personalProjects/pythonStuff/foxPython/data/vietnamese-stopwords.txt\",encoding=\"utf-8\") as f:\n",
    "    stopword_txt=f.readlines()\n",
    "    \n",
    "stopword_list=[]\n",
    "for each in stopword_txt:\n",
    "    stopword_list.append(each[:-1])\n",
    "\n",
    "with open(\"C:/Projects/personalProjects/pythonStuff/foxPython/NewTagger/posdata.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    mpos=json.load(f)\n",
    "\n",
    "responses_dict={}\n",
    "for i in data[\"intents\"]:\n",
    "    responses_dict[i[\"tag\"]]=i[\"responses\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class tạo tách từ \n",
    "class FTokenizer():\n",
    "    def __init__(self,bi_gram_list):\n",
    "        self.bi_gram_list=bi_gram_list\n",
    "\n",
    "    def bi_gram_checker(self,bi_gram_word):\n",
    "        for i in self.bi_gram_list:\n",
    "            if (bi_gram_word==i):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def fit_on_text(self,data):\n",
    "        word_to_index={}\n",
    "        index_to_word={}\n",
    "        word_to_index[\"<PAD>\"]=0\n",
    "        word_to_index[\"<OOV>\"]=1\n",
    "        index_to_word[0]=\"<PAD>\"\n",
    "        index_to_word[1]=\"<OOV>\"\n",
    "        myContinue=False\n",
    "        tokens=[]\n",
    "        for eachseq in data:\n",
    "            listtokens=eachseq.lower().split(\" \")\n",
    "            listtokens.append(\"\")\n",
    "            for i in range(0,len(listtokens)-1):\n",
    "                if (myContinue):\n",
    "                    myContinue=False\n",
    "                    continue\n",
    "                if (listtokens[i+1]!=\"\"):\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\n",
    "                    if (self.bi_gram_checker(pairtext)):\n",
    "                        myContinue=True\n",
    "                        if (tokens.count(pairtext)==0):\n",
    "                            tokens.append(pairtext)\n",
    "                if (myContinue==False):  \n",
    "                    if (tokens.count(listtokens[i])==0):\n",
    "                        tokens.append(listtokens[i])\n",
    "        for index,word in enumerate(tokens):\n",
    "            word_to_index[word]=index+2\n",
    "            index_to_word[index+2]=word\n",
    "        self.word_to_index=word_to_index\n",
    "        self.index_to_word=index_to_word\n",
    "\n",
    "    def fit_on_text2(self,data):\n",
    "        word_to_index={}\n",
    "        index_to_word={}\n",
    "        word_to_index[\"<PAD>\"]=0\n",
    "        word_to_index[\"<OOV>\"]=1\n",
    "        index_to_word[0]=\"<PAD>\"\n",
    "        index_to_word[1]=\"<OOV>\"\n",
    "        myContinue=False\n",
    "        tokens=[]\n",
    "        for eachseq in data:\n",
    "            listtokens=eachseq.lower().split(\" \")\n",
    "            listtokens.append(\"\")\n",
    "            for i in range(0,len(listtokens)-1):\n",
    "                if (listtokens[i+1]!=\"\"):\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\n",
    "                    if (self.bi_gram_checker(pairtext)):\n",
    "                        if (tokens.count(pairtext)==0):\n",
    "                            tokens.append(pairtext)\n",
    "                if (tokens.count(listtokens[i])==0 and listtokens[i]!=\"\"):\n",
    "                    tokens.append(listtokens[i])\n",
    "        for index,word in enumerate(tokens):\n",
    "            word_to_index[word]=index+2\n",
    "            index_to_word[index+2]=word\n",
    "        self.word_to_index=word_to_index\n",
    "        self.index_to_word=index_to_word\n",
    "    \n",
    "    def get_word_to_index(self):\n",
    "        return self.word_to_index\n",
    "    \n",
    "    def get_index_to_word(self):\n",
    "        return self.index_to_word\n",
    "\n",
    "    def get_max_length(self,data):\n",
    "        maxsen=0\n",
    "        for i in data:\n",
    "            if maxsen<len(i):\n",
    "                maxsen=len(i)\n",
    "        return maxsen\n",
    "\n",
    "    def text_to_sequence(self,textdata):\n",
    "        d_continue=False\n",
    "        output=[]\n",
    "        for text in textdata:\n",
    "            text=text.lower().split(\" \")\n",
    "            text.append(\"\")\n",
    "            sth=[]\n",
    "            d_continue=False\n",
    "            for i in range(0,len(text)-1):\n",
    "                if d_continue==True:\n",
    "                    d_continue=False\n",
    "                    continue\n",
    "                if (text[i+1]!=\"\"):\n",
    "                    pairtext=text[i]+\"_\"+text[i+1]\n",
    "                    for j in self.word_to_index:\n",
    "                        if pairtext.lower()==j:\n",
    "                            sth.append(self.word_to_index[j])\n",
    "                            d_continue=True\n",
    "                            break\n",
    "                if d_continue==False:\n",
    "                    for count,j in enumerate(self.word_to_index):\n",
    "                        if text[i].lower()==j:\n",
    "                            sth.append(self.word_to_index[j])\n",
    "                            break\n",
    "                        if count==len(self.word_to_index)-1:\n",
    "                            sth.append(1)\n",
    "            output.append(sth)\n",
    "        return output\n",
    "    \n",
    "    def intseq_to_index(self,sequence):\n",
    "        output=[]\n",
    "        for i in sequence:\n",
    "            output.append(self.index_to_word[i])\n",
    "        return output\n",
    "\n",
    "    def pad_sequence(self,datasequence,maxlen):\n",
    "        for i in datasequence:\n",
    "            while len(i)!=maxlen:\n",
    "                i.insert(0,0)\n",
    "        return datasequence\n",
    "\n",
    "    def bi_gram_ize(self,text):\n",
    "        listtokens=text.lower().split(\" \")\n",
    "        listtokens.append(\"\")\n",
    "        myContinue=False\n",
    "        tokens=[]\n",
    "        for i in range(0,len(listtokens)-1):\n",
    "            if (myContinue):\n",
    "                myContinue=False\n",
    "                continue\n",
    "            if (listtokens[i+1]!=\"\"):\n",
    "                pairtext=listtokens[i]+\"_\"+listtokens[i+1]\n",
    "                if (self.bi_gram_checker(pairtext)):\n",
    "                    myContinue=True\n",
    "                    if (tokens.count(pairtext)==0):\n",
    "                        tokens.append(pairtext)\n",
    "            if (myContinue==False):  \n",
    "                if (tokens.count(listtokens[i])==0):\n",
    "                    tokens.append(listtokens[i])\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VNTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,oov_token=\"<UNK>\")\n",
    "# VNTokenizer.fit_on_texts(patternlist)\n",
    "# pattern_tokenized=VNTokenizer.texts_to_sequences(patternlist)\n",
    "# pattern_preprocessed=tf.keras.preprocessing.sequence.pad_sequences(pattern_tokenized)\n",
    "# mlen=pattern_preprocessed.shape[1]\n",
    "# tokens_size=len(VNTokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Model tách từ vào các data intent,POS\n",
    "\n",
    "myTokenizer=FTokenizer(bi_gram_list)\n",
    "myTokenizer.fit_on_text2(patternlist)\n",
    "pattern_tokenized2=myTokenizer.text_to_sequence(patternlist)\n",
    "pattern_preprocessed2=tf.keras.preprocessing.sequence.pad_sequences(pattern_tokenized2)\n",
    "mlen2=pattern_preprocessed2.shape[1]\n",
    "\n",
    "posmesTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token=\"<UNK>\")\n",
    "posmesTokenizer.fit_on_texts(mpos[\"messages\"])\n",
    "mes_Tokenized=posmesTokenizer.texts_to_sequences(mpos[\"messages\"])\n",
    "finalmes=tf.keras.preprocessing.sequence.pad_sequences(mes_Tokenized)\n",
    "\n",
    "postagTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
    "postagTokenizer.fit_on_texts(mpos[\"pos\"])\n",
    "pos_tokenized=postagTokenizer.texts_to_sequences(mpos[\"pos\"])\n",
    "finalpos=tf.keras.preprocessing.sequence.pad_sequences(pos_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7877"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load weight cho Embedding\n",
    "with open(\"D:/a-Code/Ignite Niek/foxPython/word_2_vec/data/new_vocabs_weights4.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    vocab_weights=json.load(f)\n",
    "len(vocab_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 187 words (14 misses)\n"
     ]
    }
   ],
   "source": [
    "#Đưa weight vào Embedding layer\n",
    "num_tokens = len(myTokenizer.word_to_index)\n",
    "embedding_dim = 128\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in myTokenizer.word_to_index.items():\n",
    "    embedding_vector = vocab_weights.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMModel():\n",
    "    int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    # x=embedding_layer(int_sequences_input)\n",
    "    x=tf.keras.layers.Embedding(len(myTokenizer.word_to_index),128)(int_sequences_input)\n",
    "    x=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(x)\n",
    "    x=tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "    x=tf.keras.layers.Dense(16,activation=\"relu\")(x)\n",
    "    x=tf.keras.layers.Dense(len(labellist),activation=\"softmax\")(x)\n",
    "    return tf.keras.Model(int_sequences_input,x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LSTMModel()\n",
    "model.compile(tf.keras.optimizers.Adam(learning_rate=0.02),loss=\"categorical_crossentropy\",metrics=\"categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "8/8 [==============================] - 7s 9ms/step - loss: 2.6389 - categorical_accuracy: 0.0603\n",
      "Epoch 2/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.3794 - categorical_accuracy: 0.1983\n",
      "Epoch 3/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.7802 - categorical_accuracy: 0.4310\n",
      "Epoch 4/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.8546 - categorical_accuracy: 0.7672\n",
      "Epoch 5/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4752 - categorical_accuracy: 0.8621\n",
      "Epoch 6/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1968 - categorical_accuracy: 0.9569\n",
      "Epoch 7/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0778 - categorical_accuracy: 0.9914\n",
      "Epoch 8/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0651 - categorical_accuracy: 0.9828\n",
      "Epoch 9/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0245 - categorical_accuracy: 1.0000\n",
      "Epoch 10/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0263 - categorical_accuracy: 0.9914\n",
      "Epoch 11/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0048 - categorical_accuracy: 1.0000\n",
      "Epoch 12/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0036 - categorical_accuracy: 1.0000\n",
      "Epoch 13/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0024 - categorical_accuracy: 1.0000\n",
      "Epoch 14/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "Epoch 15/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 16/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.2732e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 17/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.8581e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 18/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9913e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 19/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.3329e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 20/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.8544e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 21/250\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.3873e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 22/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.0328e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 23/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.7071e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 24/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.4094e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 25/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.1430e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 26/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.9151e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 27/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6983e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 28/250\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4991e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 29/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3288e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 30/250\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1572e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 31/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0184e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 32/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.8816e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 33/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.7629e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 34/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.6531e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 35/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.5414e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 36/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.4458e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 37/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.3494e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 38/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.2673e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 39/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.1834e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 40/250\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0983e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 41/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.0228e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 42/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.9529e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 43/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8887e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 44/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.8259e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 45/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.7665e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 46/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.7106e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 47/250\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.6531e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 48/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.5992e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 49/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5517e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 50/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5044e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 51/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4618e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 52/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.4169e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 53/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3776e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 54/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3388e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 55/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3024e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 56/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2680e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 57/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2347e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 58/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2014e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 59/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1708e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 60/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.1397e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 61/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1106e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 62/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0843e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 63/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0563e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 64/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0337e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 65/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0069e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 66/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.8324e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 67/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 9.5991e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 68/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.3820e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 69/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.1781e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 70/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.9749e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 71/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.7787e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 72/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.5849e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 73/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.4070e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 74/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.2310e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 75/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.0578e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 76/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.8738e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 77/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.7022e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 78/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.5480e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 79/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.3823e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 80/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.2399e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 81/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.0845e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 82/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9431e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 83/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8123e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 84/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.6904e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 85/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.5571e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 86/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.4441e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 87/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.3261e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 88/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.2125e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 89/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.1013e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 90/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.9908e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 91/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.8829e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 92/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.7845e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 93/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.6854e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 94/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.5938e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 95/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.4993e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 96/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.4126e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 97/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.3251e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 98/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.2413e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 99/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.1604e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 100/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.0736e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 101/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.9936e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 102/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.9097e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 103/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.8329e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 104/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.7610e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 105/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.6813e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 106/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.6089e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 107/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.5359e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 108/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.4668e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 109/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.4039e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 110/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.3379e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 111/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.2742e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 112/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.2111e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 113/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.1540e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 114/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0934e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 115/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.0361e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 116/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.9817e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 117/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.9258e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 118/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8703e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 119/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.8172e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 120/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7690e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 121/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.7183e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 122/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6650e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 123/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6107e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 124/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5651e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 125/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5151e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 126/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.4689e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 127/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4242e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 128/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3815e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 129/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.3345e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 130/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2927e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 131/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2486e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 132/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2070e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 133/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1641e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 134/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1265e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 135/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0865e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 136/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0493e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 137/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0113e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 138/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9742e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 139/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9358e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 140/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.8978e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 141/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.8637e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 142/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.8298e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 143/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.7987e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 144/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.7623e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 145/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.7301e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 146/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6997e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 147/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6678e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 148/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6359e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 149/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.6033e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 150/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.5739e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 151/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.5446e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 152/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.5163e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 153/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.4868e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 154/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.4583e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 155/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.4283e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 156/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.4018e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 157/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.3744e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 158/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.3462e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 159/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.3210e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 160/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.2955e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 161/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.2717e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 162/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.2440e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 163/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.2231e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 164/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.1973e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 165/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.1748e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 166/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.1519e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 167/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.1278e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 168/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.1047e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 169/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.0844e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 170/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.0625e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 171/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.0396e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 172/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.0196e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 173/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.9978e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 174/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.9787e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 175/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.9595e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 176/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.9381e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 177/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.9205e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 178/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.9022e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 179/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.8820e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 180/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8633e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 181/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.8438e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 182/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8254e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 183/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.8071e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 184/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.7886e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 185/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.7715e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 186/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.7538e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 187/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.7365e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 188/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.7203e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 189/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.7041e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 190/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.6873e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 191/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.6695e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 192/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.6549e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 193/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.6389e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 194/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.6243e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 195/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.6089e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 196/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5934e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 197/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5798e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 198/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5636e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 199/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5498e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 200/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.5354e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 201/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5208e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 202/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.5081e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 203/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4926e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 204/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.4798e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 205/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4660e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 206/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.4537e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 207/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.4395e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 208/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4275e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 209/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.4149e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 210/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.4026e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 211/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.3909e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 212/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3778e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 213/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3671e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 214/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3567e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 215/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3433e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 216/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3321e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 217/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3199e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 218/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3097e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 219/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2980e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 220/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2882e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 221/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2763e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 222/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.2652e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 223/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2549e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 224/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.2447e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 225/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2344e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 226/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2242e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 227/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2137e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 228/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2038e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 229/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.1938e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 230/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.1828e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 231/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.1730e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 232/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1634e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 233/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1527e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 234/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.1441e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 235/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1340e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 236/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1259e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 237/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1157e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 238/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.1064e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 239/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0978e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 240/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.0894e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 241/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0805e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 242/250\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0724e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 243/250\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.0635e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 244/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0554e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 245/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.0458e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 246/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0381e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 247/250\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0297e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 248/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0214e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 249/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0134e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 250/250\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0060e-05 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x222f3eb8b20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pattern_preprocessed2,labelsOH,batch_size=16,epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagModel=tf.keras.models.load_model(\"E:/MLCourse/TF2/Notebook/SmartFennec/NER file/TagModel\")\n",
    "POStagger=tf.keras.models.load_model(\"D:/a-Code/Ignite Niek/foxPython/NewTagger/PosTaggerModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"E:/MLCourse/TF2/Notebook/SmartFennec/NER file/tagtrain.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "#     data=json.load(f)\n",
    "# textlist=[]\n",
    "\n",
    "# for each in data:\n",
    "#     textlist.append(each[\"sentence\"])\n",
    "\n",
    "# TextTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,lower=True,oov_token=\"<UNK>\")\n",
    "# TextTokenizer.fit_on_texts(textlist)\n",
    "\n",
    "# tokenizedText=TextTokenizer.texts_to_sequences(textlist)\n",
    "# textFinal=tf.keras.preprocessing.sequence.pad_sequences(tokenizedText)\n",
    "\n",
    "# def insert_text(text):\n",
    "#     text_tokenized=TextTokenizer.texts_to_sequences([text])\n",
    "#     finaltext=tf.keras.preprocessing.sequence.pad_sequences(text_tokenized,maxlen=textFinal.shape[1])\n",
    "#     return finaltext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entitiesOutput(rs):\n",
    "#     txt=[]\n",
    "#     for i in rs[0]:\n",
    "#         txt.append(tagdict[np.argmax(i)])\n",
    "#         # use argmax \n",
    "#     return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mình sẽ đưa mấy hoạt động được kích hoạt ở đây\n",
    "def action_activate(input,entities):\n",
    "    if input==\"question_from_user\":\n",
    "        if len(entities)!=0:\n",
    "            noun=pos_filter(entities,\"noun\")\n",
    "            s=\"\"\n",
    "            for i in noun:\n",
    "                s=s+i[0].replace(\"_\",\" \")+\" \"\n",
    "            return wiki.summary(s)\n",
    "        return \"Sorry, hiện tại mình chưa biết nữa\"\n",
    "\n",
    "    if input==\"askingme\":\n",
    "        noun=pos_filter(entities,\"noun\")\n",
    "        verb=pos_filter(entities,\"verb\")\n",
    "        known_list=[\"sách\",\"Toán\",\"Văn\",\"Anh\",\"Minecraft\",\"LOL\"]\n",
    "        for i in known_list:\n",
    "            if noun[0].count(i.lower())!=0:\n",
    "                return \"Yess\"\n",
    "        return \"Nah\"\n",
    "    return \"//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cho Bot(tạm thời)\n",
    "class ShortTermMemory():\n",
    "    def __init__(self):\n",
    "        self.mem={}\n",
    "        self.dialogue_count=0\n",
    "        self.length_limit=20\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mem={}\n",
    "    \n",
    "    def add(self,intent,sentence_entities):\n",
    "        self.mem[self.dialogue_count]=[intent,sentence_entities]\n",
    "        self.dialogue_count=self.dialogue_count+1\n",
    "        if self.dialogue_count==self.length_limit:\n",
    "            self.mem={}\n",
    "\n",
    "    def lookup(self):\n",
    "        return self.mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chấp nhận acc trên n% \n",
    "def accept_accuracy(pred,acc):\n",
    "    if (max(pred)>acc):\n",
    "        return np.argmax(pred)\n",
    "    else:\n",
    "        return len(pred)\n",
    "#Enable PrintMode\n",
    "def debugMode(printlist):\n",
    "    for count,i in enumerate(printlist):\n",
    "        print(count,\"/\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert từ text qua số\n",
    "posmesTokenizer.index_word[0]=\"<PAD>\"\n",
    "postagTokenizer.index_word[0]=\"<PAD>\"\n",
    "def insert_text(inp):\n",
    "    inp_gramized=myTokenizer.bi_gram_ize(inp)\n",
    "    inp_tokenized=posmesTokenizer.texts_to_sequences([inp_gramized])\n",
    "    final_inp=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=finalmes.shape[1])\n",
    "    return final_inp\n",
    "#Xử lý output data, biến thành dạng [Từ,POS]\n",
    "def posAnalyze(inp,posdata):\n",
    "    pos=[]\n",
    "    for i in posdata[0]:\n",
    "        if postagTokenizer.index_word[np.argmax(i)]!=\"<PAD>\":\n",
    "            pos.append(postagTokenizer.index_word[np.argmax(i)])\n",
    "    l=[]\n",
    "    for i in range(0,len(pos)):\n",
    "        if pos[i]!=\"<PAD>\":\n",
    "            l.append([inp[i],pos[i]])\n",
    "    return l\n",
    "#Filter loại từ\n",
    "def pos_filter(pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\n",
    "    pos_filtered=[]\n",
    "    for i in pos_rs:\n",
    "        if (i[1]==pos_name and stopword.count(i[0])==0):\n",
    "            pos_filtered.append(i)\n",
    "    return pos_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "from random import randint\n",
    "cred = credentials.Certificate(\"D:/a-Code/Ignite Niek/foxPython/ChatBot/android-firebase-1f1b8-firebase-adminsdk-wtlm2-7a0d32b4fc.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(sentence):\n",
    "    #Example format\n",
    "    def sentences(sub,item):\n",
    "        sen1 = [f\"Bạn đã thử {sub} {item} chưa?\",\n",
    "        f\"Mình thấy {sub} {item} cũng được lắm nha!\",\n",
    "        f\"Nhiều người thích {sub} {item} lắm đó!\"]\n",
    "        return sen1[randint(0,2)]\n",
    "    #Things that db contains\n",
    "    names = []\n",
    "    docs = db.collection(u'bot').stream()\n",
    "    for doc in docs:\n",
    "        names.append(doc.id)\n",
    "    #Check if the asked thing in list\n",
    "    for word in sentence.split():\n",
    "        if word in names:\n",
    "            doc_ref = db.collection(u'bot').document(word)\n",
    "            doc = doc_ref.get()\n",
    "            if doc.exists:\n",
    "                dictz = doc.to_dict()\n",
    "                return(sentences(word,dictz[\"item\"][randint(0,len(dictz[\"item\"])-1)]))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "User:  hi banj\n",
      "Fennec:  Là sao vậy ? Mình không hiểu cậu lắm !\n",
      "Fennec db :  None\n",
      "//\n",
      "\n",
      "User:  bạn thích coi phim gì\n",
      "Fennec:  Check it out\n",
      "Mình thấy phim Parasite cũng được lắm nha!\n",
      "Fennec db :  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file D:\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "ename": "DisambiguationError",
     "evalue": "\"Phim (định hướng)\" may refer to: \nĐiện ảnh\nPhim truyền hình\nPhim chiếu mạng\nPhim điện ảnh\nĐiện ảnh truyền hình\nPhim video\nPhim băng đĩa\nPhim nhựa\nPhim hoạt hình\nPhim tình cảm\nRạp chiếu phim\nKỹ thuật điện ảnh\nQuá trình làm phim\n^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDisambiguationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cd112292c5f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"this conversation memory: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-cd112292c5f9>\u001b[0m in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fennec: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fennec db : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_activate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_rs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"this conversation memory: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-e85b4ca83931>\u001b[0m in \u001b[0;36maction_activate\u001b[1;34m(input, entities)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnoun\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mwiki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"Sorry, hiện tại mình chưa biết nữa\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\wikipedia\\util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(title, sentences, chars, auto_suggest, redirect)\u001b[0m\n\u001b[0;32m    229\u001b[0m   \u001b[1;31m# use auto_suggest and redirect to get the correct article\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;31m# also, use page's error checking to raise DisambiguationError if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m   \u001b[0mpage_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauto_suggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m   \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mpageid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    391\u001b[0m       \u001b[0mmay_refer_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_lis\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mDisambiguationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmay_refer_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDisambiguationError\u001b[0m: \"Phim (định hướng)\" may refer to: \nĐiện ảnh\nPhim truyền hình\nPhim chiếu mạng\nPhim điện ảnh\nĐiện ảnh truyền hình\nPhim video\nPhim băng đĩa\nPhim nhựa\nPhim hoạt hình\nPhim tình cảm\nRạp chiếu phim\nKỹ thuật điện ảnh\nQuá trình làm phim\n^"
     ]
    }
   ],
   "source": [
    "postagTokenizer.index_word[0]=\"<PAD>\"\n",
    "wiki.set_lang(\"vi\")\n",
    "labelsre=np.array(list(labelsOH.columns)+[\"NULL\"])\n",
    "def chat():\n",
    "    memory=ShortTermMemory()\n",
    "    print(\"start\")\n",
    "    while True:\n",
    "        reply=\"\"\n",
    "        inp=input(\"You: \")\n",
    "        if inp==\"quit\":\n",
    "            break\n",
    "        inp_tokenized=myTokenizer.text_to_sequence([inp])\n",
    "        inp_preprocessed=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=mlen2)\n",
    "        pos_pred=POStagger.predict(insert_text(inp))\n",
    "        pos_rs=posAnalyze(myTokenizer.bi_gram_ize(inp),pos_pred)\n",
    "        rs=np.round(model.predict(inp_preprocessed),decimals=2)\n",
    "        pred=labelsre[accept_accuracy(rs[0],0.8)]\n",
    "        print(\"User: \",inp)\n",
    "        #debugMode([inp,myTokenizer.intseq_to_index(inp_preprocessed[0]),pos_rs,pred,pos_filter(pos_rs,\"noun\"),pos_filter(pos_rs,\"verb\")])\n",
    "        memory.add(pred,pos_rs)\n",
    "        if (pred!=\"askingme\" and pred!=\"NULL\"):\n",
    "            reply=random.choice(responses_dict[pred])\n",
    "        if pred==\"NULL\":\n",
    "            reply=\"Là sao vậy ? Mình không hiểu cậu lắm !\"\n",
    "        print(\"Fennec: \",reply)\n",
    "        print(\"Fennec db : \",checker(inp))\n",
    "        print(action_activate(pred,pos_rs))\n",
    "        print(\"\")\n",
    "    print(\"this conversation memory: \",memory.lookup())\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issue: (1)khi được hỏi, bot sẽ cư xử sao ?\n",
    "        (2) Khi nào mình sẽ đưa data vào brain để sử dụng sau\n",
    "        (3) Structure trả lời sẽ như thế nào để phù hợp với (1)\n",
    "        (4) Limit của cấu trúc này là bot sẽ rất bị động\n",
    "        (5) intents sao mà Bot đạt tối ưu nhất có thể\n",
    "        (6) Nếu entities liên kết với nhau thì mình sẽ có gì\n",
    "        (7) Bot decide được không"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)Solution:\n",
    "Mỗi situation sẽ có một dictionary để fully kích hoạt\n",
    "{asking -> {@chủ_đề->[@main, @vấn_đề}]}\n",
    "Ví dụ: {askingme -> {studying->[môn học, vấn đề được hỏi}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)Tạm thời là mọi thông tin (dưới dạng dictionary), để search thông tin thì mình sẽ đo sự tương quan giữa hai câu (có thể từ entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) có thể sẽ liên quan đến data của (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) có thể random hỏi user nếu user có đặc điểm nào đó đáng lưu ý"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Environment: Chat dialogue ,Action: Chat với người dùng, Reward: user's sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44a47853b1918eb710e25fd6ba6ee9af6fac694ece621a4756b81ff23436e8c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
