{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "import tensorflow as tf\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "import tensorflow_text as tf_text\r\n",
    "import FTokenizer\r\n",
    "import random\r\n",
    "import wikipedia as wiki"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# Open data Intent,Bi_gram,VN Stop word,posData\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/ChatBot/intentsVNver1.json\",encoding=\"utf-8\") as file:\r\n",
    "    data=json.load(file)\r\n",
    "patternlist=[]\r\n",
    "labels=[]\r\n",
    "for dt in data[\"intents\"]:\r\n",
    "    for pattern in dt[\"patterns\"]:\r\n",
    "        patternlist.append(pattern)\r\n",
    "        labels.append(dt[\"tag\"])\r\n",
    "labels\r\n",
    "labels=np.array(labels)\r\n",
    "labelsOH=pd.get_dummies(labels)\r\n",
    "labellist=list(labelsOH)\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/data/bi_grams.txt\",encoding=\"utf-8\") as f:\r\n",
    "    bi_gramtxt=f.read()\r\n",
    "bi_gram_list=[]\r\n",
    "for each in bi_gramtxt.split(\",\"):\r\n",
    "    each=each[:-1]\r\n",
    "    each=each[2:]\r\n",
    "    each=each.replace(\" \",\"_\")\r\n",
    "    bi_gram_list.append(each)\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/data/vietnamese-stopwords.txt\",encoding=\"utf-8\") as f:\r\n",
    "    stopword_txt=f.readlines()\r\n",
    "    \r\n",
    "stopword_list=[]\r\n",
    "for each in stopword_txt:\r\n",
    "    stopword_list.append(each[:-1])\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/NewTagger/posdata.json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "    mpos=json.load(f)\r\n",
    "\r\n",
    "responses_dict={}\r\n",
    "for i in data[\"intents\"]:\r\n",
    "    responses_dict[i[\"tag\"]]=i[\"responses\"]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "#Class tạo tách từ \r\n",
    "class FTokenizer():\r\n",
    "    def __init__(self,bi_gram_list):\r\n",
    "        self.bi_gram_list=bi_gram_list\r\n",
    "\r\n",
    "    def bi_gram_checker(self,bi_gram_word):\r\n",
    "        for i in self.bi_gram_list:\r\n",
    "            if (bi_gram_word==i):\r\n",
    "                return True\r\n",
    "        return False\r\n",
    "    \r\n",
    "    def fit_on_text(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (myContinue):\r\n",
    "                    myContinue=False\r\n",
    "                    continue\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        myContinue=True\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (myContinue==False):  \r\n",
    "                    if (tokens.count(listtokens[i])==0):\r\n",
    "                        tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "\r\n",
    "    def fit_on_text2(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (tokens.count(listtokens[i])==0 and listtokens[i]!=\"\"):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "    \r\n",
    "    def get_word_to_index(self):\r\n",
    "        return self.word_to_index\r\n",
    "    \r\n",
    "    def get_index_to_word(self):\r\n",
    "        return self.index_to_word\r\n",
    "\r\n",
    "    def get_max_length(self,data):\r\n",
    "        maxsen=0\r\n",
    "        for i in data:\r\n",
    "            if maxsen<len(i):\r\n",
    "                maxsen=len(i)\r\n",
    "        return maxsen\r\n",
    "\r\n",
    "    def text_to_sequence(self,textdata):\r\n",
    "        d_continue=False\r\n",
    "        output=[]\r\n",
    "        for text in textdata:\r\n",
    "            text=text.lower().split(\" \")\r\n",
    "            text.append(\"\")\r\n",
    "            sth=[]\r\n",
    "            d_continue=False\r\n",
    "            for i in range(0,len(text)-1):\r\n",
    "                if d_continue==True:\r\n",
    "                    d_continue=False\r\n",
    "                    continue\r\n",
    "                if (text[i+1]!=\"\"):\r\n",
    "                    pairtext=text[i]+\"_\"+text[i+1]\r\n",
    "                    for j in self.word_to_index:\r\n",
    "                        if pairtext.lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            d_continue=True\r\n",
    "                            break\r\n",
    "                if d_continue==False:\r\n",
    "                    for count,j in enumerate(self.word_to_index):\r\n",
    "                        if text[i].lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            break\r\n",
    "                        if count==len(self.word_to_index)-1:\r\n",
    "                            sth.append(1)\r\n",
    "            output.append(sth)\r\n",
    "        return output\r\n",
    "    \r\n",
    "    def intseq_to_index(self,sequence):\r\n",
    "        output=[]\r\n",
    "        for i in sequence:\r\n",
    "            output.append(self.index_to_word[i])\r\n",
    "        return output\r\n",
    "\r\n",
    "    def pad_sequence(self,datasequence,maxlen):\r\n",
    "        for i in datasequence:\r\n",
    "            while len(i)!=maxlen:\r\n",
    "                i.insert(0,0)\r\n",
    "        return datasequence\r\n",
    "\r\n",
    "    def bi_gram_ize(self,text):\r\n",
    "        listtokens=text.lower().split(\" \")\r\n",
    "        listtokens.append(\"\")\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for i in range(0,len(listtokens)-1):\r\n",
    "            if (myContinue):\r\n",
    "                myContinue=False\r\n",
    "                continue\r\n",
    "            if (listtokens[i+1]!=\"\"):\r\n",
    "                pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                if (self.bi_gram_checker(pairtext)):\r\n",
    "                    myContinue=True\r\n",
    "                    if (tokens.count(pairtext)==0):\r\n",
    "                        tokens.append(pairtext)\r\n",
    "            if (myContinue==False):  \r\n",
    "                if (tokens.count(listtokens[i])==0):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "        return tokens\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# VNTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,oov_token=\"<UNK>\")\r\n",
    "# VNTokenizer.fit_on_texts(patternlist)\r\n",
    "# pattern_tokenized=VNTokenizer.texts_to_sequences(patternlist)\r\n",
    "# pattern_preprocessed=tf.keras.preprocessing.sequence.pad_sequences(pattern_tokenized)\r\n",
    "# mlen=pattern_preprocessed.shape[1]\r\n",
    "# tokens_size=len(VNTokenizer.word_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "#Apply Model tách từ vào các data intent,POS\r\n",
    "\r\n",
    "myTokenizer=FTokenizer(bi_gram_list)\r\n",
    "myTokenizer.fit_on_text2(patternlist)\r\n",
    "pattern_tokenized2=myTokenizer.text_to_sequence(patternlist)\r\n",
    "pattern_preprocessed2=tf.keras.preprocessing.sequence.pad_sequences(pattern_tokenized2)\r\n",
    "mlen2=pattern_preprocessed2.shape[1]\r\n",
    "\r\n",
    "posmesTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token=\"<UNK>\")\r\n",
    "posmesTokenizer.fit_on_texts(mpos[\"messages\"])\r\n",
    "mes_Tokenized=posmesTokenizer.texts_to_sequences(mpos[\"messages\"])\r\n",
    "finalmes=tf.keras.preprocessing.sequence.pad_sequences(mes_Tokenized)\r\n",
    "\r\n",
    "postagTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000)\r\n",
    "postagTokenizer.fit_on_texts(mpos[\"pos\"])\r\n",
    "pos_tokenized=postagTokenizer.texts_to_sequences(mpos[\"pos\"])\r\n",
    "finalpos=tf.keras.preprocessing.sequence.pad_sequences(pos_tokenized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "#Load weight cho Embedding\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/word_2_vec/data/new_vocabs_weights4.json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "    vocab_weights=json.load(f)\r\n",
    "len(vocab_weights)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7877"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "#Đưa weight vào Embedding layer\r\n",
    "num_tokens = len(myTokenizer.word_to_index)\r\n",
    "embedding_dim = 128\r\n",
    "hits = 0\r\n",
    "misses = 0\r\n",
    "\r\n",
    "# Prepare embedding matrix\r\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\r\n",
    "for word, i in myTokenizer.word_to_index.items():\r\n",
    "    embedding_vector = vocab_weights.get(word)\r\n",
    "    if embedding_vector is not None:\r\n",
    "        # Words not found in embedding index will be all-zeros.\r\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\r\n",
    "        embedding_matrix[i] = embedding_vector\r\n",
    "        hits += 1\r\n",
    "    else:\r\n",
    "        misses += 1\r\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\r\n",
    "\r\n",
    "embedding_layer = tf.keras.layers.Embedding(\r\n",
    "    num_tokens,\r\n",
    "    embedding_dim,\r\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\r\n",
    "    trainable=False,\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 187 words (14 misses)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def LSTMModel():\r\n",
    "    int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\r\n",
    "    x=embedding_layer(int_sequences_input)\r\n",
    "    x=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(x)\r\n",
    "    x=tf.keras.layers.GlobalMaxPool1D()(x)\r\n",
    "    x=tf.keras.layers.Dense(16,activation=\"relu\")(x)\r\n",
    "    x=tf.keras.layers.Dense(len(labellist),activation=\"softmax\")(x)\r\n",
    "    return tf.keras.Model(int_sequences_input,x)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "model=LSTMModel()\r\n",
    "model.compile(tf.keras.optimizers.Adam(learning_rate=0.02),loss=\"categorical_crossentropy\",metrics=\"categorical_accuracy\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "model.fit(pattern_preprocessed2,labelsOH,batch_size=16,epochs=250)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/250\n",
      "8/8 [==============================] - 4s 9ms/step - loss: 2.4660 - categorical_accuracy: 0.1466\n",
      "Epoch 2/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.5204 - categorical_accuracy: 0.4828\n",
      "Epoch 3/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6665 - categorical_accuracy: 0.8017\n",
      "Epoch 4/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2725 - categorical_accuracy: 0.9310\n",
      "Epoch 5/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2049 - categorical_accuracy: 0.9569\n",
      "Epoch 6/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0950 - categorical_accuracy: 0.9741\n",
      "Epoch 7/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0551 - categorical_accuracy: 0.9828\n",
      "Epoch 8/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0467 - categorical_accuracy: 0.9828\n",
      "Epoch 9/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0398 - categorical_accuracy: 0.9828\n",
      "Epoch 10/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0434 - categorical_accuracy: 0.9828\n",
      "Epoch 11/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0382 - categorical_accuracy: 0.9828\n",
      "Epoch 12/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0355 - categorical_accuracy: 0.9741\n",
      "Epoch 13/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0410 - categorical_accuracy: 0.9828\n",
      "Epoch 14/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0390 - categorical_accuracy: 0.9828\n",
      "Epoch 15/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0351 - categorical_accuracy: 0.9828\n",
      "Epoch 16/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0333 - categorical_accuracy: 0.9828\n",
      "Epoch 17/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0333 - categorical_accuracy: 0.9828\n",
      "Epoch 18/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0326 - categorical_accuracy: 0.9828\n",
      "Epoch 19/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0335 - categorical_accuracy: 0.9741\n",
      "Epoch 20/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0321 - categorical_accuracy: 0.9655\n",
      "Epoch 21/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0346 - categorical_accuracy: 0.9828\n",
      "Epoch 22/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0355 - categorical_accuracy: 0.9828\n",
      "Epoch 23/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0490 - categorical_accuracy: 0.9828\n",
      "Epoch 24/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0319 - categorical_accuracy: 0.9828\n",
      "Epoch 25/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0317 - categorical_accuracy: 0.9828\n",
      "Epoch 26/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0326 - categorical_accuracy: 0.9828\n",
      "Epoch 27/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0338 - categorical_accuracy: 0.9741\n",
      "Epoch 28/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0324 - categorical_accuracy: 0.9741\n",
      "Epoch 29/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0354 - categorical_accuracy: 0.9828\n",
      "Epoch 30/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 31/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0376 - categorical_accuracy: 0.9655\n",
      "Epoch 32/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0406 - categorical_accuracy: 0.9741\n",
      "Epoch 33/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0432 - categorical_accuracy: 0.9828\n",
      "Epoch 34/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0374 - categorical_accuracy: 0.9828\n",
      "Epoch 35/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0322 - categorical_accuracy: 0.9828\n",
      "Epoch 36/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0395 - categorical_accuracy: 0.9828\n",
      "Epoch 37/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 38/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0313 - categorical_accuracy: 0.9828\n",
      "Epoch 39/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0339 - categorical_accuracy: 0.9828\n",
      "Epoch 40/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0324 - categorical_accuracy: 0.9828\n",
      "Epoch 41/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 42/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0314 - categorical_accuracy: 0.9828\n",
      "Epoch 43/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0309 - categorical_accuracy: 0.9828\n",
      "Epoch 44/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0339 - categorical_accuracy: 0.9828\n",
      "Epoch 45/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 46/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0321 - categorical_accuracy: 0.9655\n",
      "Epoch 47/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0273 - categorical_accuracy: 0.9828\n",
      "Epoch 48/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0369 - categorical_accuracy: 0.9828\n",
      "Epoch 49/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0371 - categorical_accuracy: 0.9828\n",
      "Epoch 50/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0292 - categorical_accuracy: 0.9828\n",
      "Epoch 51/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0492 - categorical_accuracy: 0.9741\n",
      "Epoch 52/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0444 - categorical_accuracy: 0.9741\n",
      "Epoch 53/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0337 - categorical_accuracy: 0.9828\n",
      "Epoch 54/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 55/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - categorical_accuracy: 0.9828\n",
      "Epoch 56/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0333 - categorical_accuracy: 0.9828\n",
      "Epoch 57/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 58/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 59/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0373 - categorical_accuracy: 0.9741\n",
      "Epoch 60/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0363 - categorical_accuracy: 0.9741\n",
      "Epoch 61/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0367 - categorical_accuracy: 0.9828\n",
      "Epoch 62/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0333 - categorical_accuracy: 0.9828\n",
      "Epoch 63/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 64/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0310 - categorical_accuracy: 0.9828\n",
      "Epoch 65/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0308 - categorical_accuracy: 0.9828\n",
      "Epoch 66/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0299 - categorical_accuracy: 0.9828\n",
      "Epoch 67/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0309 - categorical_accuracy: 0.9828\n",
      "Epoch 68/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0316 - categorical_accuracy: 0.9828\n",
      "Epoch 69/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 70/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0307 - categorical_accuracy: 0.9828\n",
      "Epoch 71/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0333 - categorical_accuracy: 0.9828\n",
      "Epoch 72/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0351 - categorical_accuracy: 0.9655\n",
      "Epoch 73/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0396 - categorical_accuracy: 0.9741\n",
      "Epoch 74/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0333 - categorical_accuracy: 0.9741\n",
      "Epoch 75/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0320 - categorical_accuracy: 0.9828\n",
      "Epoch 76/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0307 - categorical_accuracy: 0.9828\n",
      "Epoch 77/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0304 - categorical_accuracy: 0.9828\n",
      "Epoch 78/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0319 - categorical_accuracy: 0.9828\n",
      "Epoch 79/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0299 - categorical_accuracy: 0.9828\n",
      "Epoch 80/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - categorical_accuracy: 0.9828\n",
      "Epoch 81/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 82/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0307 - categorical_accuracy: 0.9828\n",
      "Epoch 83/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 84/250\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0339 - categorical_accuracy: 0.9828\n",
      "Epoch 85/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 86/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0320 - categorical_accuracy: 0.9828\n",
      "Epoch 87/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0316 - categorical_accuracy: 0.9828\n",
      "Epoch 88/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0355 - categorical_accuracy: 0.9828\n",
      "Epoch 89/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 90/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0337 - categorical_accuracy: 0.9828\n",
      "Epoch 91/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0324 - categorical_accuracy: 0.9828\n",
      "Epoch 92/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 93/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0300 - categorical_accuracy: 0.9828\n",
      "Epoch 94/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 95/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 96/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0314 - categorical_accuracy: 0.9828\n",
      "Epoch 97/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0303 - categorical_accuracy: 0.9828\n",
      "Epoch 98/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0313 - categorical_accuracy: 0.9828\n",
      "Epoch 99/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 100/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0315 - categorical_accuracy: 0.9828\n",
      "Epoch 101/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 102/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0314 - categorical_accuracy: 0.9828\n",
      "Epoch 103/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 104/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0331 - categorical_accuracy: 0.9828\n",
      "Epoch 105/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0327 - categorical_accuracy: 0.9828\n",
      "Epoch 106/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0316 - categorical_accuracy: 0.9828\n",
      "Epoch 107/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0418 - categorical_accuracy: 0.9655\n",
      "Epoch 108/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0403 - categorical_accuracy: 0.9741\n",
      "Epoch 109/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0352 - categorical_accuracy: 0.9741\n",
      "Epoch 110/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0303 - categorical_accuracy: 0.9828\n",
      "Epoch 111/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0310 - categorical_accuracy: 0.9828\n",
      "Epoch 112/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0323 - categorical_accuracy: 0.9828\n",
      "Epoch 113/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0316 - categorical_accuracy: 0.9828\n",
      "Epoch 114/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0371 - categorical_accuracy: 0.9655\n",
      "Epoch 115/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0320 - categorical_accuracy: 0.9828\n",
      "Epoch 116/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0303 - categorical_accuracy: 0.9828\n",
      "Epoch 117/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 118/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 119/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 120/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0303 - categorical_accuracy: 0.9828\n",
      "Epoch 121/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 122/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 123/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 124/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0307 - categorical_accuracy: 0.9828\n",
      "Epoch 125/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0309 - categorical_accuracy: 0.9828\n",
      "Epoch 126/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 127/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0294 - categorical_accuracy: 0.9828\n",
      "Epoch 128/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0318 - categorical_accuracy: 0.9828\n",
      "Epoch 129/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0305 - categorical_accuracy: 0.9828\n",
      "Epoch 130/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 131/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 132/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 133/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0309 - categorical_accuracy: 0.9828\n",
      "Epoch 134/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 135/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 136/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0300 - categorical_accuracy: 0.9828\n",
      "Epoch 137/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 138/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 139/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0308 - categorical_accuracy: 0.9828\n",
      "Epoch 140/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 141/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 142/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 143/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0294 - categorical_accuracy: 0.9828\n",
      "Epoch 144/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0303 - categorical_accuracy: 0.9828\n",
      "Epoch 145/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0338 - categorical_accuracy: 0.9828\n",
      "Epoch 146/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0335 - categorical_accuracy: 0.9828\n",
      "Epoch 147/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0306 - categorical_accuracy: 0.9828\n",
      "Epoch 148/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0356 - categorical_accuracy: 0.9828\n",
      "Epoch 149/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0317 - categorical_accuracy: 0.9828\n",
      "Epoch 150/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0304 - categorical_accuracy: 0.9828\n",
      "Epoch 151/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0304 - categorical_accuracy: 0.9828\n",
      "Epoch 152/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0310 - categorical_accuracy: 0.9828\n",
      "Epoch 153/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 154/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0306 - categorical_accuracy: 0.9828\n",
      "Epoch 155/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0290 - categorical_accuracy: 0.9828\n",
      "Epoch 156/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0315 - categorical_accuracy: 0.9828\n",
      "Epoch 157/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0310 - categorical_accuracy: 0.9828\n",
      "Epoch 158/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0299 - categorical_accuracy: 0.9828\n",
      "Epoch 159/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0293 - categorical_accuracy: 0.9828\n",
      "Epoch 160/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0300 - categorical_accuracy: 0.9828\n",
      "Epoch 161/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0307 - categorical_accuracy: 0.9828\n",
      "Epoch 162/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 163/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 164/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0333 - categorical_accuracy: 0.9828\n",
      "Epoch 165/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0315 - categorical_accuracy: 0.9828\n",
      "Epoch 166/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 167/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0306 - categorical_accuracy: 0.9828\n",
      "Epoch 168/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0327 - categorical_accuracy: 0.9828\n",
      "Epoch 169/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0303 - categorical_accuracy: 0.9828\n",
      "Epoch 170/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0309 - categorical_accuracy: 0.9828\n",
      "Epoch 171/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0294 - categorical_accuracy: 0.9828\n",
      "Epoch 172/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 173/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0305 - categorical_accuracy: 0.9828\n",
      "Epoch 174/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 175/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0310 - categorical_accuracy: 0.9828\n",
      "Epoch 176/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0300 - categorical_accuracy: 0.9828\n",
      "Epoch 177/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0304 - categorical_accuracy: 0.9828\n",
      "Epoch 178/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 179/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 180/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 181/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0294 - categorical_accuracy: 0.9828\n",
      "Epoch 182/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0312 - categorical_accuracy: 0.9828\n",
      "Epoch 183/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 184/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 185/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 186/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 187/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 188/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0299 - categorical_accuracy: 0.9828\n",
      "Epoch 189/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0316 - categorical_accuracy: 0.9828\n",
      "Epoch 190/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0302 - categorical_accuracy: 0.9741\n",
      "Epoch 191/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0349 - categorical_accuracy: 0.9741\n",
      "Epoch 192/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0328 - categorical_accuracy: 0.9741\n",
      "Epoch 193/250\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0304 - categorical_accuracy: 0.9914\n",
      "Epoch 194/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 195/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0309 - categorical_accuracy: 0.9655\n",
      "Epoch 196/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0311 - categorical_accuracy: 0.9741\n",
      "Epoch 197/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0320 - categorical_accuracy: 0.9828\n",
      "Epoch 198/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 199/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0294 - categorical_accuracy: 0.9828\n",
      "Epoch 200/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0300 - categorical_accuracy: 0.9828\n",
      "Epoch 201/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0300 - categorical_accuracy: 0.9828\n",
      "Epoch 202/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 203/250\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0293 - categorical_accuracy: 0.9828\n",
      "Epoch 204/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0299 - categorical_accuracy: 0.9828\n",
      "Epoch 205/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0319 - categorical_accuracy: 0.9828\n",
      "Epoch 206/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 207/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 208/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0315 - categorical_accuracy: 0.9828\n",
      "Epoch 209/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0292 - categorical_accuracy: 0.9828\n",
      "Epoch 210/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 211/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0318 - categorical_accuracy: 0.9828\n",
      "Epoch 212/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0294 - categorical_accuracy: 0.9828\n",
      "Epoch 213/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0293 - categorical_accuracy: 0.9828\n",
      "Epoch 214/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0296 - categorical_accuracy: 0.9828\n",
      "Epoch 215/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 216/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0293 - categorical_accuracy: 0.9828\n",
      "Epoch 217/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0292 - categorical_accuracy: 0.9828\n",
      "Epoch 218/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 219/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0299 - categorical_accuracy: 0.9828\n",
      "Epoch 220/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0324 - categorical_accuracy: 0.9741\n",
      "Epoch 221/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0311 - categorical_accuracy: 0.9741\n",
      "Epoch 222/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0309 - categorical_accuracy: 0.9741\n",
      "Epoch 223/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0295 - categorical_accuracy: 0.9828\n",
      "Epoch 224/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0305 - categorical_accuracy: 0.9828\n",
      "Epoch 225/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 226/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0305 - categorical_accuracy: 0.9828\n",
      "Epoch 227/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 228/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0302 - categorical_accuracy: 0.9741\n",
      "Epoch 229/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0316 - categorical_accuracy: 0.9741\n",
      "Epoch 230/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0346 - categorical_accuracy: 0.9741\n",
      "Epoch 231/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0311 - categorical_accuracy: 0.9655\n",
      "Epoch 232/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0311 - categorical_accuracy: 0.9828\n",
      "Epoch 233/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0311 - categorical_accuracy: 0.9828\n",
      "Epoch 234/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0305 - categorical_accuracy: 0.9828\n",
      "Epoch 235/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0314 - categorical_accuracy: 0.9828\n",
      "Epoch 236/250\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 237/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 238/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0287 - categorical_accuracy: 0.9828\n",
      "Epoch 239/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0308 - categorical_accuracy: 0.9828\n",
      "Epoch 240/250\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0308 - categorical_accuracy: 0.9828\n",
      "Epoch 241/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0311 - categorical_accuracy: 0.9828\n",
      "Epoch 242/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0311 - categorical_accuracy: 0.9828\n",
      "Epoch 243/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 244/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0305 - categorical_accuracy: 0.9828\n",
      "Epoch 245/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0298 - categorical_accuracy: 0.9828\n",
      "Epoch 246/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0304 - categorical_accuracy: 0.9828\n",
      "Epoch 247/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0301 - categorical_accuracy: 0.9828\n",
      "Epoch 248/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0297 - categorical_accuracy: 0.9828\n",
      "Epoch 249/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0302 - categorical_accuracy: 0.9828\n",
      "Epoch 250/250\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0300 - categorical_accuracy: 0.9828\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20674509d60>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "#tagModel=tf.keras.models.load_model(\"E:/MLCourse/TF2/Notebook/SmartFennec/NER file/TagModel\")\r\n",
    "POStagger=tf.keras.models.load_model(\"E:/MLCourse/foxBot/foxPython/NewTagger/PosTaggerModel\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# with open(\"E:/MLCourse/TF2/Notebook/SmartFennec/NER file/tagtrain.json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "#     data=json.load(f)\r\n",
    "# textlist=[]\r\n",
    "\r\n",
    "# for each in data:\r\n",
    "#     textlist.append(each[\"sentence\"])\r\n",
    "\r\n",
    "# TextTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,lower=True,oov_token=\"<UNK>\")\r\n",
    "# TextTokenizer.fit_on_texts(textlist)\r\n",
    "\r\n",
    "# tokenizedText=TextTokenizer.texts_to_sequences(textlist)\r\n",
    "# textFinal=tf.keras.preprocessing.sequence.pad_sequences(tokenizedText)\r\n",
    "\r\n",
    "# def insert_text(text):\r\n",
    "#     text_tokenized=TextTokenizer.texts_to_sequences([text])\r\n",
    "#     finaltext=tf.keras.preprocessing.sequence.pad_sequences(text_tokenized,maxlen=textFinal.shape[1])\r\n",
    "#     return finaltext\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# def entitiesOutput(rs):\r\n",
    "#     txt=[]\r\n",
    "#     for i in rs[0]:\r\n",
    "#         txt.append(tagdict[np.argmax(i)])\r\n",
    "#         # use argmax \r\n",
    "#     return txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "#Mình sẽ đưa mấy hoạt động được kích hoạt ở đây\r\n",
    "def action_activate(input,entities):\r\n",
    "    if input==\"question_from_user\":\r\n",
    "        if len(entities)!=0:\r\n",
    "            noun=pos_filter(entities,\"noun\")\r\n",
    "            s=\"\"\r\n",
    "            for i in noun:\r\n",
    "                s=s+i[0].replace(\"_\",\" \")+\" \"\r\n",
    "            return wiki.summary(s)\r\n",
    "        return \"Sorry, hiện tại mình chưa biết nữa\"\r\n",
    "\r\n",
    "    if input==\"askingme\":\r\n",
    "        noun=pos_filter(entities,\"noun\")\r\n",
    "        verb=pos_filter(entities,\"verb\")\r\n",
    "        known_list=[\"sách\",\"Toán\",\"Văn\",\"Anh\",\"Minecraft\",\"LOL\"]\r\n",
    "        for i in known_list:\r\n",
    "            if noun[0].count(i.lower())!=0:\r\n",
    "                return \"Yess\"\r\n",
    "        return \"Nah\"\r\n",
    "    return \"//\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# Memory cho Bot(tạm thời)\r\n",
    "class ShortTermMemory():\r\n",
    "    def __init__(self):\r\n",
    "        self.mem={}\r\n",
    "        self.dialogue_count=0\r\n",
    "        self.length_limit=20\r\n",
    "    \r\n",
    "    def reset(self):\r\n",
    "        self.mem={}\r\n",
    "    \r\n",
    "    def add(self,intent,sentence_entities):\r\n",
    "        self.mem[self.dialogue_count]=[intent,sentence_entities]\r\n",
    "        self.dialogue_count=self.dialogue_count+1\r\n",
    "        if self.dialogue_count==self.length_limit:\r\n",
    "            self.mem={}\r\n",
    "\r\n",
    "    def lookup(self):\r\n",
    "        return self.mem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "#Chấp nhận acc trên n% \r\n",
    "def accept_accuracy(pred,acc):\r\n",
    "    if (max(pred)>acc):\r\n",
    "        return np.argmax(pred)\r\n",
    "    else:\r\n",
    "        return len(pred)\r\n",
    "#Enable PrintMode\r\n",
    "def debugMode(printlist):\r\n",
    "    for count,i in enumerate(printlist):\r\n",
    "        print(count,\"/\",i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "#Convert từ text qua số\r\n",
    "posmesTokenizer.index_word[0]=\"<PAD>\"\r\n",
    "postagTokenizer.index_word[0]=\"<PAD>\"\r\n",
    "def insert_text(inp):\r\n",
    "    inp_gramized=myTokenizer.bi_gram_ize(inp)\r\n",
    "    inp_tokenized=posmesTokenizer.texts_to_sequences([inp_gramized])\r\n",
    "    final_inp=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=finalmes.shape[1])\r\n",
    "    return final_inp\r\n",
    "#Xử lý output data, biến thành dạng [Từ,POS]\r\n",
    "def posAnalyze(inp,posdata):\r\n",
    "    pos=[]\r\n",
    "    for i in posdata[0]:\r\n",
    "        if postagTokenizer.index_word[np.argmax(i)]!=\"<PAD>\":\r\n",
    "            pos.append(postagTokenizer.index_word[np.argmax(i)])\r\n",
    "    l=[]\r\n",
    "    for i in range(0,len(pos)):\r\n",
    "        if pos[i]!=\"<PAD>\":\r\n",
    "            l.append([inp[i],pos[i]])\r\n",
    "    return l\r\n",
    "#Filter loại từ\r\n",
    "def pos_filter(pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\r\n",
    "    pos_filtered=[]\r\n",
    "    for i in pos_rs:\r\n",
    "        if (i[1]==pos_name and stopword.count(i[0])==0):\r\n",
    "            pos_filtered.append(i)\r\n",
    "    return pos_filtered"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "postagTokenizer.index_word[0]=\"<PAD>\"\r\n",
    "wiki.set_lang(\"vi\")\r\n",
    "labelsre=np.array(list(labelsOH.columns)+[\"NULL\"])\r\n",
    "def chat():\r\n",
    "    memory=ShortTermMemory()\r\n",
    "    print(\"start\")\r\n",
    "    while True:\r\n",
    "        reply=\"\"\r\n",
    "        inp=input(\"You: \")\r\n",
    "        if inp==\"quit\":\r\n",
    "            break\r\n",
    "        inp_tokenized=myTokenizer.text_to_sequence([inp])\r\n",
    "        inp_preprocessed=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=mlen2)\r\n",
    "        pos_pred=POStagger.predict(insert_text(inp))\r\n",
    "        pos_rs=posAnalyze(myTokenizer.bi_gram_ize(inp),pos_pred)\r\n",
    "        rs=np.round(model.predict(inp_preprocessed),decimals=2)\r\n",
    "        pred=labelsre[accept_accuracy(rs[0],0.8)]\r\n",
    "        print(\"User: \",inp)\r\n",
    "        #debugMode([inp,myTokenizer.intseq_to_index(inp_preprocessed[0]),pos_rs,pred,pos_filter(pos_rs,\"noun\"),pos_filter(pos_rs,\"verb\")])\r\n",
    "        memory.add(pred,pos_rs)\r\n",
    "        if (pred!=\"askingme\" and pred!=\"NULL\"):\r\n",
    "            reply=random.choice(responses_dict[pred])\r\n",
    "        if pred==\"NULL\":\r\n",
    "            reply=\"Là sao vậy ? Mình không hiểu cậu lắm !\"\r\n",
    "        print(\"Fennec: \",reply)\r\n",
    "        print(action_activate(pred,pos_rs))\r\n",
    "        print(\"\")\r\n",
    "    print(\"this conversation memory: \",memory.lookup())\r\n",
    "chat()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start\n",
      "User:  Cậu thích minecraft không\n",
      "Fennec:  \n",
      "Yess\n",
      "\n",
      "User:  bạn biết Lenin không\n",
      "Fennec:  \n",
      "Nah\n",
      "\n",
      "this conversation memory:  {0: ['askingme', [['cậu', 'noun'], ['thích', 'verb'], ['minecraft', 'noun'], ['không', 'x']]], 1: ['askingme', [['bạn', 'noun'], ['biết', 'verb'], ['lenin', 'noun'], ['không', 'x']]]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "issue: (1)khi được hỏi, bot sẽ cư xử sao ?\r\n",
    "        (2) Khi nào mình sẽ đưa data vào brain để sử dụng sau\r\n",
    "        (3) Structure trả lời sẽ như thế nào để phù hợp với (1)\r\n",
    "        (4) Limit của cấu trúc này là bot sẽ rất bị động\r\n",
    "        (5) intents sao mà Bot đạt tối ưu nhất có thể\r\n",
    "        (6) Nếu entities liên kết với nhau thì mình sẽ có gì\r\n",
    "        (7) Bot decide được không"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1)Solution:\r\n",
    "Mỗi situation sẽ có một dictionary để fully kích hoạt\r\n",
    "{asking -> {@chủ_đề->[@main, @vấn_đề}]}\r\n",
    "Ví dụ: {askingme -> {studying->[môn học, vấn đề được hỏi}]}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(2)Tạm thời là mọi thông tin (dưới dạng dictionary), để search thông tin thì mình sẽ đo sự tương quan giữa hai câu (có thể từ entities)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(3) có thể sẽ liên quan đến data của (5)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(4) có thể random hỏi user nếu user có đặc điểm nào đó đáng lưu ý"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(7) Environment: Chat dialogue ,Action: Chat với người dùng, Reward: user's sentiment"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "3d95097792c4c1f8621d6eabaad3727eb6f403675c9e5393220cb43082419a68"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}