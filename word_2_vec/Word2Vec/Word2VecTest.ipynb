{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "3d95097792c4c1f8621d6eabaad3727eb6f403675c9e5393220cb43082419a68"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import nltk\r\n",
    "import numpy as np \r\n",
    "import pandas as pd \r\n",
    "import seaborn as sns\r\n",
    "import json\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_text as tf_text\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "with open(\"E:/MLCourse/TF2/Notebook/SmartFennec/text\\MathSentences.json\",encoding=\"utf-8\") as f:\r\n",
    "    data=json.load(f)\r\n",
    "wikidata=data[\"sentences\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "with open (\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data (1).json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "    message=json.load(f)\r\n",
    "MessageKeys=list(message.keys())\r\n",
    "queslist=[]\r\n",
    "anslist=[]\r\n",
    "messageData=[]\r\n",
    "for eachkey in MessageKeys:\r\n",
    "    for question in message[eachkey][\"câu hỏi\"]:\r\n",
    "        queslist.append(question)\r\n",
    "    for answer in message[eachkey][\"trả lời\"]:\r\n",
    "        anslist.append(answer)\r\n",
    "for i,j in zip(queslist,anslist):\r\n",
    "    messageData.append(i+\" \"+j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "bigdata=messageData+wikidata"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5980"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "with open(\"E:/MLCourse/TF2/Notebook/SmartFennec/bi_grams.txt\",encoding=\"utf-8\") as f:\r\n",
    "    bi_gramtxt=f.read()\r\n",
    "bi_gram_list=[]\r\n",
    "for each in bi_gramtxt.split(\",\"):\r\n",
    "    each=each[:-1]\r\n",
    "    each=each[2:]\r\n",
    "    bi_gram_list.append(each)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "basicTokenizer=tf_text.WhitespaceTokenizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# # Check if already in bigramdict\r\n",
    "# def fit_on_text(text,dict):\r\n",
    "#     d_continue=False\r\n",
    "#     sth=[]\r\n",
    "#     for i in range(0,len(text)):\r\n",
    "#         if d_continue==True:\r\n",
    "#             d_continue=False\r\n",
    "#             continue\r\n",
    "#         try:\r\n",
    "#             pairtext=text[i]+\" \"+text[i+1]\r\n",
    "#         except:\r\n",
    "#             pairtext=\"\"\r\n",
    "#         # if i!=len(textlist):\r\n",
    "#         #     pairtext=textlist[i]+\" \"+textlist[i+1]\r\n",
    "#         for j in dict:\r\n",
    "#             if pairtext.lower()==j:\r\n",
    "#                 sth.append(dict[j])\r\n",
    "#                 d_continue=True\r\n",
    "#                 break\r\n",
    "#         if d_continue==False:\r\n",
    "#             for count,j in enumerate(dict):\r\n",
    "#                 if text[i].lower()==j:\r\n",
    "#                     sth.append(dict[j])\r\n",
    "#                     break\r\n",
    "#                 if count==len(dict)-1:\r\n",
    "#                     sth.append(0)\r\n",
    "#     return sth\r\n",
    "#Pad sequence\r\n",
    "def pad_sequence(datasequence):\r\n",
    "    maxsen=0\r\n",
    "    for i in datasequence:\r\n",
    "        if maxsen<len(i):\r\n",
    "            maxsen=len(i)\r\n",
    "\r\n",
    "    for i in datasequence:\r\n",
    "        while len(i)!=maxsen:\r\n",
    "            i.insert(0,0)\r\n",
    "    return np.array(dataseq)\r\n",
    "#seq_to_word\r\n",
    "\r\n",
    "# def seq_to_word(data):\r\n",
    "#     tokens=[]\r\n",
    "#     for eachpattern in data:\r\n",
    "#         for eachword in eachpattern.split():\r\n",
    "#             tokens.append(eachword.lower())\r\n",
    "#     bi_grams=tf_text.ngrams(tokens,2,reduction_type=tf_text.Reduction.STRING_JOIN,string_separator=' ')\r\n",
    "#     bg=np.array(bi_grams)\r\n",
    "#     listwordbg=[]\r\n",
    "#     # for i in bigramstring.split():\r\n",
    "#     #     listwordbg.append(i)\r\n",
    "#     listword=[]\r\n",
    "#     for i in bg:\r\n",
    "#         listword.append(i.decode(\"utf-8\"))\r\n",
    "#     chosenwords=[]\r\n",
    "#     finaltokens=[]\r\n",
    "#         # single word\r\n",
    "#     for i in tokens:\r\n",
    "#         check=True\r\n",
    "#         for j in listwordbg:\r\n",
    "#             if i==j:\r\n",
    "#                 check=False\r\n",
    "#                 break\r\n",
    "#         if (finaltokens.count(i)==0) and (i!=\"?\") and (i!=\"thế\") and (i!=\"vậy\") and (i!=\"nè\") and (check):\r\n",
    "#             finaltokens.append(i)\r\n",
    "#         # bigram\r\n",
    "#     for i in listword:\r\n",
    "#         for j in bi_gram_list:\r\n",
    "#             if (i==j) and (chosenwords.count(i)==0):\r\n",
    "#                 chosenwords.append(i)\r\n",
    "#     wordlist=finaltokens+chosenwords\r\n",
    "#     print(chosenwords)\r\n",
    "#     dict={}\r\n",
    "#     for word in wordlist:\r\n",
    "#         for stopword in stopwords:\r\n",
    "#             if (word==stopword):\r\n",
    "#                 wordlist.remove(word)\r\n",
    "#     for i,j in enumerate(wordlist):\r\n",
    "#         dict[j]=i+1\r\n",
    "#         if j==\"\":\r\n",
    "#             continue\r\n",
    "#     return dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def bi_gram_checker(bi_gram_word):\r\n",
    "    for i in bi_gram_list:\r\n",
    "        if (bi_gram_word==i):\r\n",
    "            return True\r\n",
    "    return False\r\n",
    "\r\n",
    "def seq2word(data):\r\n",
    "    word_to_index={}\r\n",
    "    index_to_word={}\r\n",
    "    myContinue=False\r\n",
    "    tokens=[]\r\n",
    "    for eachseq in data:\r\n",
    "        listtokens=eachseq.lower().split(\" \")\r\n",
    "        listtokens.append(\"\")\r\n",
    "        for i in range(0,len(listtokens)-1):\r\n",
    "            if (myContinue):\r\n",
    "                myContinue=False\r\n",
    "                continue\r\n",
    "            if (listtokens[i+1]!=\"\"):\r\n",
    "                pairtext=listtokens[i]+\" \"+listtokens[i+1]\r\n",
    "                if (bi_gram_checker(pairtext)):\r\n",
    "                    myContinue=True\r\n",
    "                    if (tokens.count(pairtext)==0):\r\n",
    "                        tokens.append(pairtext)\r\n",
    "            if (myContinue==False):  \r\n",
    "                if (tokens.count(listtokens[i])==0):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "    for index,word in enumerate(tokens):\r\n",
    "        word_to_index[word]=index+1\r\n",
    "        index_to_word[index+1]=word\r\n",
    "    return word_to_index,index_to_word\r\n",
    "\r\n",
    "def fitontext(textdata,dict):\r\n",
    "    d_continue=False\r\n",
    "    output=[]\r\n",
    "    for text in textdata:\r\n",
    "        text=text.lower().split(\" \")\r\n",
    "        text.append(\"\")\r\n",
    "        sth=[]\r\n",
    "        for i in range(0,len(text)-1):\r\n",
    "            if d_continue==True:\r\n",
    "                d_continue=False\r\n",
    "                continue\r\n",
    "            if (text[i+1]!=\"\"):\r\n",
    "                pairtext=text[i]+\" \"+text[i+1]\r\n",
    "            for j in dict:\r\n",
    "                if pairtext.lower()==j:\r\n",
    "                    sth.append(dict[j])\r\n",
    "                    d_continue=True\r\n",
    "                    break\r\n",
    "            if d_continue==False:\r\n",
    "                for count,j in enumerate(dict):\r\n",
    "                    if text[i].lower()==j:\r\n",
    "                        sth.append(dict[j])\r\n",
    "                        break\r\n",
    "                    if count==len(dict)-1:\r\n",
    "                        sth.append(0)\r\n",
    "        output.append(sth)\r\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "worddict,indexofword=seq2word(bigdata)\r\n",
    "len(worddict)\r\n",
    "dataseq=fitontext(bigdata,worddict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# dataseq=[]\r\n",
    "# basicTokenizer=tf_text.WhitespaceTokenizer()\r\n",
    "# for eachseq in data[\"sentences\"]:\r\n",
    "#     tokenstext=np.array(basicTokenizer.tokenize(eachseq))\r\n",
    "#     textlistt=[]\r\n",
    "#     for i in tokenstext:\r\n",
    "#         if (i.decode(\"utf-8\")!=\"?\") and (i.decode(\"utf-8\")!=\"thế\") and (i.decode(\"utf-8\")!=\"vậy\") and (i.decode(\"utf-8\")!=\"nè\") and (i.decode(\"utf-8\")!=\"quá đi\") and (i.decode(\"utf-8\")!=\"vậy đi\") and (i.decode(\"utf-8\")!=\"á\") and (i.decode(\"utf-8\")!=\"với\"):\r\n",
    "#             textlistt.append(i.decode(\"utf-8\"))\r\n",
    "#     # print(eachseq)\r\n",
    "#     seqfit=fit_on_text(textlistt,worddict)\r\n",
    "#     dataseq.append(seqfit)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "seqX=pad_sequence(dataseq)\r\n",
    "seqX\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    4,    5,    6],\n",
       "       [   0,    0,    0, ...,   11,   12,   13],\n",
       "       [   0,    0,    0, ...,   17,   18,   19],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  369,  477,  477],\n",
       "       [   0,    0,    0, ...,  369, 3411, 3411],\n",
       "       [   0,    0,    0, ...,    0,    0, 3411]])"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sampling_table=tf.keras.preprocessing.sequence.make_sampling_table(size=10)\r\n",
    "sampling_table"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00315225, 0.00315225, 0.00547597, 0.00741556, 0.00912817,\n",
       "       0.01068435, 0.01212381, 0.01347162, 0.01474487, 0.0159558 ])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# Whole thing\r\n",
    "def generate_training_data(sequences,window_size,num_ns,vocab_size,seed):\r\n",
    "    targets,contexts,labels=[],[],[]\r\n",
    "    sampling_table=tf.keras.preprocessing.sequence.make_sampling_table(size=vocab_size)\r\n",
    "    for sequence in sequences:\r\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "            sequence,\r\n",
    "            vocabulary_size=vocab_size,\r\n",
    "            window_size=window_size,\r\n",
    "            negative_samples=0)\r\n",
    "        for target_word,context_word in positive_skip_grams:\r\n",
    "            context_class=tf.expand_dims(tf.constant([context_word],dtype=\"int64\"),1)\r\n",
    "            negative_sampling_candidates,_,_=tf.random.log_uniform_candidate_sampler(\r\n",
    "                true_classes=context_class,\r\n",
    "                num_true=1,\r\n",
    "                num_sampled=num_ns,\r\n",
    "                unique=True,\r\n",
    "                range_max=vocab_size,\r\n",
    "                seed=seed,\r\n",
    "                name=\"negative_sampling\"\r\n",
    "            )\r\n",
    "            negative_sampling_candidates=tf.expand_dims(negative_sampling_candidates,1)\r\n",
    "            context=tf.concat([context_class,negative_sampling_candidates],0)\r\n",
    "            label=tf.constant([1]+[0]*num_ns,dtype=\"int64\")\r\n",
    "            targets.append(target_word)\r\n",
    "            contexts.append(context)\r\n",
    "            labels.append(label)\r\n",
    "    return targets, contexts, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# # Create window\r\n",
    "# window_size=2\r\n",
    "# positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "#     seqX.tolist()[0],\r\n",
    "#     vocabulary_size=len(worddict),\r\n",
    "#     window_size=window_size,\r\n",
    "#     negative_samples=0\r\n",
    "# )\r\n",
    "# print(len(positive_skip_grams))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# # Negative Sampling\r\n",
    "# target_word, context_word=positive_skip_grams[6]\r\n",
    "# num_ns=4\r\n",
    "# SEED=42\r\n",
    "# context_class=tf.reshape(tf.constant(context_word,dtype=\"int64\"),(1,1))\r\n",
    "# negative_sampling_candidates,_,_=tf.random.log_uniform_candidate_sampler(\r\n",
    "#     true_classes=context_class,\r\n",
    "#     num_true=1,\r\n",
    "#     num_sampled=num_ns,\r\n",
    "#     unique=True,\r\n",
    "#     range_max=len(worddict),\r\n",
    "#     seed=SEED,\r\n",
    "#     name=\"negative_sampling\"\r\n",
    "# )\r\n",
    "\r\n",
    "# negative_sampling_candidates"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 51,   9, 173,  21], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# negative_sampling_candidates=tf.expand_dims(negative_sampling_candidates,1)\r\n",
    "# context=tf.concat([context_class,negative_sampling_candidates],0)\r\n",
    "# label=tf.constant([1]+[0]*num_ns,dtype=\"int64\")\r\n",
    "\r\n",
    "# target=tf.squeeze(target_word)\r\n",
    "# context=tf.squeeze(context)\r\n",
    "# label=tf.squeeze(label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "targets,contexts,labels=generate_training_data(\r\n",
    "    seqX,\r\n",
    "    window_size=3,\r\n",
    "    num_ns=4,\r\n",
    "    vocab_size=len(worddict)+1,\r\n",
    "    seed=42\r\n",
    ")\r\n",
    "num_ns=4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "len(worddict)\r\n",
    "len(labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "451170"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "BATCH_SIZE = 500\r\n",
    "BUFFER_SIZE = 10000\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\r\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: (((500,), (500, 5, 1)), (500, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "from tensorflow.keras import Model\r\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "class Word2Vec(Model):\r\n",
    "  def __init__(self, vocab_size, embedding_dim):\r\n",
    "    super(Word2Vec, self).__init__()\r\n",
    "    self.target_embedding = Embedding(vocab_size,\r\n",
    "                                      embedding_dim,\r\n",
    "                                      input_length=1,\r\n",
    "                                      name=\"w2v_embedding\")\r\n",
    "    self.context_embedding = Embedding(vocab_size,\r\n",
    "                                       embedding_dim,\r\n",
    "                                       input_length=num_ns+1)\r\n",
    "    self.dots = Dot(axes=(3, 2))\r\n",
    "    self.flatten = Flatten()\r\n",
    "\r\n",
    "  def call(self, pair):\r\n",
    "    target, context = pair\r\n",
    "    word_emb = self.target_embedding(target)\r\n",
    "    context_emb = self.context_embedding(context)\r\n",
    "    dots = self.dots([context_emb, word_emb])\r\n",
    "    return self.flatten(dots)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "seqX.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5980, 118)"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "def custom_loss(x_logit, y_true):\r\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "embedding_dim = 128\r\n",
    "word2vec = Word2Vec(len(worddict)+1, embedding_dim)\r\n",
    "word2vec.compile(optimizer='adam',\r\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
    "                 metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "word2vec.fit(dataset, epochs=500, callbacks=[tensorboard_callback])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/500\n",
      "902/902 [==============================] - 15s 14ms/step - loss: 0.9641 - accuracy: 0.6481\n",
      "Epoch 2/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.7769 - accuracy: 0.6967\n",
      "Epoch 3/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.7151 - accuracy: 0.7231\n",
      "Epoch 4/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.6549 - accuracy: 0.7498\n",
      "Epoch 5/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.5990 - accuracy: 0.7746\n",
      "Epoch 6/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.5496 - accuracy: 0.7967\n",
      "Epoch 7/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.5071 - accuracy: 0.8153\n",
      "Epoch 8/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.4714 - accuracy: 0.8303\n",
      "Epoch 9/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.4418 - accuracy: 0.8418\n",
      "Epoch 10/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.4172 - accuracy: 0.8513\n",
      "Epoch 11/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3969 - accuracy: 0.8585\n",
      "Epoch 12/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3801 - accuracy: 0.8645\n",
      "Epoch 13/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.3660 - accuracy: 0.8691\n",
      "Epoch 14/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.3541 - accuracy: 0.8727\n",
      "Epoch 15/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3441 - accuracy: 0.8756\n",
      "Epoch 16/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3355 - accuracy: 0.8782\n",
      "Epoch 17/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.3282 - accuracy: 0.8804\n",
      "Epoch 18/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3218 - accuracy: 0.8822\n",
      "Epoch 19/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.3163 - accuracy: 0.8838\n",
      "Epoch 20/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3115 - accuracy: 0.8852\n",
      "Epoch 21/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3072 - accuracy: 0.8862\n",
      "Epoch 22/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3034 - accuracy: 0.8872\n",
      "Epoch 23/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.3001 - accuracy: 0.8881\n",
      "Epoch 24/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2971 - accuracy: 0.8889\n",
      "Epoch 25/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2945 - accuracy: 0.8896\n",
      "Epoch 26/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2920 - accuracy: 0.8903\n",
      "Epoch 27/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2899 - accuracy: 0.8909\n",
      "Epoch 28/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2879 - accuracy: 0.8914\n",
      "Epoch 29/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2861 - accuracy: 0.8919\n",
      "Epoch 30/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2845 - accuracy: 0.8923\n",
      "Epoch 31/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2830 - accuracy: 0.8927\n",
      "Epoch 32/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2816 - accuracy: 0.8929\n",
      "Epoch 33/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2803 - accuracy: 0.8932\n",
      "Epoch 34/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2792 - accuracy: 0.8935\n",
      "Epoch 35/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2781 - accuracy: 0.8938\n",
      "Epoch 36/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2771 - accuracy: 0.8940\n",
      "Epoch 37/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2762 - accuracy: 0.8941\n",
      "Epoch 38/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2753 - accuracy: 0.8943\n",
      "Epoch 39/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2745 - accuracy: 0.8944\n",
      "Epoch 40/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2737 - accuracy: 0.8946\n",
      "Epoch 41/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2730 - accuracy: 0.8947\n",
      "Epoch 42/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2724 - accuracy: 0.8948\n",
      "Epoch 43/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2718 - accuracy: 0.8949\n",
      "Epoch 44/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2712 - accuracy: 0.8949\n",
      "Epoch 45/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2706 - accuracy: 0.8951\n",
      "Epoch 46/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2701 - accuracy: 0.8952\n",
      "Epoch 47/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2696 - accuracy: 0.8952\n",
      "Epoch 48/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2692 - accuracy: 0.8953\n",
      "Epoch 49/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2688 - accuracy: 0.8954\n",
      "Epoch 50/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2683 - accuracy: 0.8954\n",
      "Epoch 51/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2680 - accuracy: 0.8955\n",
      "Epoch 52/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2676 - accuracy: 0.8956\n",
      "Epoch 53/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2672 - accuracy: 0.8956\n",
      "Epoch 54/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2669 - accuracy: 0.8957\n",
      "Epoch 55/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2666 - accuracy: 0.8957\n",
      "Epoch 56/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2663 - accuracy: 0.8958\n",
      "Epoch 57/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2660 - accuracy: 0.8958\n",
      "Epoch 58/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2657 - accuracy: 0.8959\n",
      "Epoch 59/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2655 - accuracy: 0.8959\n",
      "Epoch 60/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2652 - accuracy: 0.8960\n",
      "Epoch 61/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2650 - accuracy: 0.8961\n",
      "Epoch 62/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2647 - accuracy: 0.8961\n",
      "Epoch 63/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2645 - accuracy: 0.8962\n",
      "Epoch 64/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2643 - accuracy: 0.8962\n",
      "Epoch 65/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2641 - accuracy: 0.8963\n",
      "Epoch 66/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2639 - accuracy: 0.8963\n",
      "Epoch 67/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2637 - accuracy: 0.8963\n",
      "Epoch 68/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2635 - accuracy: 0.8964\n",
      "Epoch 69/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2634 - accuracy: 0.8963\n",
      "Epoch 70/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2632 - accuracy: 0.8964\n",
      "Epoch 71/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2630 - accuracy: 0.8964\n",
      "Epoch 72/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2629 - accuracy: 0.8964\n",
      "Epoch 73/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2627 - accuracy: 0.8964\n",
      "Epoch 74/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2626 - accuracy: 0.8964\n",
      "Epoch 75/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2625 - accuracy: 0.8964\n",
      "Epoch 76/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2623 - accuracy: 0.8965\n",
      "Epoch 77/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2622 - accuracy: 0.8965\n",
      "Epoch 78/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2621 - accuracy: 0.8965\n",
      "Epoch 79/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2619 - accuracy: 0.8965\n",
      "Epoch 80/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2618 - accuracy: 0.8965\n",
      "Epoch 81/500\n",
      "902/902 [==============================] - 15s 16ms/step - loss: 0.2617 - accuracy: 0.8966\n",
      "Epoch 82/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2616 - accuracy: 0.8966\n",
      "Epoch 83/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2615 - accuracy: 0.8966\n",
      "Epoch 84/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2614 - accuracy: 0.8967\n",
      "Epoch 85/500\n",
      "902/902 [==============================] - 15s 16ms/step - loss: 0.2613 - accuracy: 0.8967\n",
      "Epoch 86/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2612 - accuracy: 0.8966\n",
      "Epoch 87/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2611 - accuracy: 0.8966\n",
      "Epoch 88/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2610 - accuracy: 0.8966\n",
      "Epoch 89/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2609 - accuracy: 0.8966\n",
      "Epoch 90/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2608 - accuracy: 0.8966\n",
      "Epoch 91/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2608 - accuracy: 0.8966\n",
      "Epoch 92/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2607 - accuracy: 0.8966\n",
      "Epoch 93/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2606 - accuracy: 0.8966\n",
      "Epoch 94/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2605 - accuracy: 0.8966\n",
      "Epoch 95/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2604 - accuracy: 0.8966\n",
      "Epoch 96/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2604 - accuracy: 0.8966\n",
      "Epoch 97/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2603 - accuracy: 0.8966\n",
      "Epoch 98/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2602 - accuracy: 0.8966\n",
      "Epoch 99/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2602 - accuracy: 0.8966\n",
      "Epoch 100/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2601 - accuracy: 0.8966\n",
      "Epoch 101/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2600 - accuracy: 0.8967\n",
      "Epoch 102/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2600 - accuracy: 0.8967\n",
      "Epoch 103/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2599 - accuracy: 0.8966\n",
      "Epoch 104/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2598 - accuracy: 0.8967\n",
      "Epoch 105/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2598 - accuracy: 0.8967\n",
      "Epoch 106/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2597 - accuracy: 0.8967\n",
      "Epoch 107/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2597 - accuracy: 0.8967\n",
      "Epoch 108/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2596 - accuracy: 0.8967\n",
      "Epoch 109/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2596 - accuracy: 0.8967\n",
      "Epoch 110/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2595 - accuracy: 0.8967\n",
      "Epoch 111/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2595 - accuracy: 0.8966\n",
      "Epoch 112/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2594 - accuracy: 0.8966\n",
      "Epoch 113/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2594 - accuracy: 0.8966\n",
      "Epoch 114/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2593 - accuracy: 0.8966\n",
      "Epoch 115/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2593 - accuracy: 0.8966\n",
      "Epoch 116/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2592 - accuracy: 0.8966\n",
      "Epoch 117/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2592 - accuracy: 0.8966\n",
      "Epoch 118/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2591 - accuracy: 0.8966\n",
      "Epoch 119/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2591 - accuracy: 0.8966\n",
      "Epoch 120/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2591 - accuracy: 0.8966\n",
      "Epoch 121/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2590 - accuracy: 0.8967\n",
      "Epoch 122/500\n",
      "902/902 [==============================] - 15s 16ms/step - loss: 0.2590 - accuracy: 0.8967\n",
      "Epoch 123/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2589 - accuracy: 0.8967\n",
      "Epoch 124/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2589 - accuracy: 0.8967\n",
      "Epoch 125/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2589 - accuracy: 0.8967\n",
      "Epoch 126/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2588 - accuracy: 0.8967\n",
      "Epoch 127/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2588 - accuracy: 0.8967\n",
      "Epoch 128/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2587 - accuracy: 0.8967\n",
      "Epoch 129/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2587 - accuracy: 0.8967\n",
      "Epoch 130/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2587 - accuracy: 0.8967\n",
      "Epoch 131/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2586 - accuracy: 0.8967\n",
      "Epoch 132/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2586 - accuracy: 0.8967\n",
      "Epoch 133/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2586 - accuracy: 0.8967\n",
      "Epoch 134/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2585 - accuracy: 0.8967\n",
      "Epoch 135/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2585 - accuracy: 0.8967\n",
      "Epoch 136/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2585 - accuracy: 0.8967\n",
      "Epoch 137/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2584 - accuracy: 0.8967\n",
      "Epoch 138/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2584 - accuracy: 0.8967\n",
      "Epoch 139/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2584 - accuracy: 0.8967\n",
      "Epoch 140/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2584 - accuracy: 0.8967\n",
      "Epoch 141/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2583 - accuracy: 0.8968\n",
      "Epoch 142/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2583 - accuracy: 0.8968\n",
      "Epoch 143/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2583 - accuracy: 0.8968\n",
      "Epoch 144/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2582 - accuracy: 0.8968\n",
      "Epoch 145/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2582 - accuracy: 0.8968\n",
      "Epoch 146/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2582 - accuracy: 0.8968\n",
      "Epoch 147/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2582 - accuracy: 0.8968\n",
      "Epoch 148/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2581 - accuracy: 0.8968\n",
      "Epoch 149/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2581 - accuracy: 0.8968\n",
      "Epoch 150/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2581 - accuracy: 0.8968\n",
      "Epoch 151/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2581 - accuracy: 0.8968\n",
      "Epoch 152/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2580 - accuracy: 0.8968\n",
      "Epoch 153/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2580 - accuracy: 0.8968\n",
      "Epoch 154/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2580 - accuracy: 0.8968\n",
      "Epoch 155/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2580 - accuracy: 0.8968\n",
      "Epoch 156/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2579 - accuracy: 0.8968\n",
      "Epoch 157/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2579 - accuracy: 0.8968\n",
      "Epoch 158/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2579 - accuracy: 0.8969\n",
      "Epoch 159/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2579 - accuracy: 0.8969\n",
      "Epoch 160/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2578 - accuracy: 0.8968\n",
      "Epoch 161/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2578 - accuracy: 0.8969\n",
      "Epoch 162/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2578 - accuracy: 0.8969\n",
      "Epoch 163/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2578 - accuracy: 0.8968\n",
      "Epoch 164/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2578 - accuracy: 0.8968\n",
      "Epoch 165/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2577 - accuracy: 0.8968\n",
      "Epoch 166/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2577 - accuracy: 0.8968\n",
      "Epoch 167/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2577 - accuracy: 0.8968\n",
      "Epoch 168/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2577 - accuracy: 0.8968\n",
      "Epoch 169/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2577 - accuracy: 0.8968\n",
      "Epoch 170/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2576 - accuracy: 0.8968\n",
      "Epoch 171/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2576 - accuracy: 0.8968\n",
      "Epoch 172/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2576 - accuracy: 0.8968\n",
      "Epoch 173/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2576 - accuracy: 0.8968\n",
      "Epoch 174/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2576 - accuracy: 0.8968\n",
      "Epoch 175/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2575 - accuracy: 0.8968\n",
      "Epoch 176/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2575 - accuracy: 0.8968\n",
      "Epoch 177/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2575 - accuracy: 0.8968\n",
      "Epoch 178/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2575 - accuracy: 0.8969\n",
      "Epoch 179/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2575 - accuracy: 0.8968\n",
      "Epoch 180/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2575 - accuracy: 0.8968\n",
      "Epoch 181/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2574 - accuracy: 0.8968\n",
      "Epoch 182/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2574 - accuracy: 0.8968\n",
      "Epoch 183/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2574 - accuracy: 0.8968\n",
      "Epoch 184/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2574 - accuracy: 0.8968\n",
      "Epoch 185/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2574 - accuracy: 0.8968\n",
      "Epoch 186/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2574 - accuracy: 0.8968\n",
      "Epoch 187/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2573 - accuracy: 0.8968\n",
      "Epoch 188/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2573 - accuracy: 0.8968\n",
      "Epoch 189/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2573 - accuracy: 0.8968\n",
      "Epoch 190/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2573 - accuracy: 0.8968\n",
      "Epoch 191/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2573 - accuracy: 0.8968\n",
      "Epoch 192/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2573 - accuracy: 0.8968\n",
      "Epoch 193/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2573 - accuracy: 0.8968\n",
      "Epoch 194/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2572 - accuracy: 0.8968\n",
      "Epoch 195/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2572 - accuracy: 0.8968\n",
      "Epoch 196/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2572 - accuracy: 0.8968\n",
      "Epoch 197/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2572 - accuracy: 0.8968\n",
      "Epoch 198/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2572 - accuracy: 0.8968\n",
      "Epoch 199/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2572 - accuracy: 0.8968\n",
      "Epoch 200/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2572 - accuracy: 0.8968\n",
      "Epoch 201/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2571 - accuracy: 0.8968\n",
      "Epoch 202/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2571 - accuracy: 0.8968\n",
      "Epoch 203/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2571 - accuracy: 0.8968\n",
      "Epoch 204/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2571 - accuracy: 0.8968\n",
      "Epoch 205/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2571 - accuracy: 0.8968\n",
      "Epoch 206/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2571 - accuracy: 0.8968\n",
      "Epoch 207/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2571 - accuracy: 0.8968\n",
      "Epoch 208/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2570 - accuracy: 0.8968\n",
      "Epoch 209/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2570 - accuracy: 0.8969\n",
      "Epoch 210/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2570 - accuracy: 0.8969\n",
      "Epoch 211/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2570 - accuracy: 0.8969\n",
      "Epoch 212/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2570 - accuracy: 0.8969\n",
      "Epoch 213/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2570 - accuracy: 0.8969\n",
      "Epoch 214/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2570 - accuracy: 0.8969\n",
      "Epoch 215/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2570 - accuracy: 0.8969\n",
      "Epoch 216/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2569 - accuracy: 0.8969\n",
      "Epoch 217/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2569 - accuracy: 0.8969\n",
      "Epoch 218/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2569 - accuracy: 0.8969\n",
      "Epoch 219/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2569 - accuracy: 0.8968\n",
      "Epoch 220/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2569 - accuracy: 0.8968\n",
      "Epoch 221/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2569 - accuracy: 0.8969\n",
      "Epoch 222/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2569 - accuracy: 0.8969\n",
      "Epoch 223/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2569 - accuracy: 0.8969\n",
      "Epoch 224/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2569 - accuracy: 0.8969\n",
      "Epoch 225/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 226/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 227/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 228/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 229/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 230/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 231/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 232/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 233/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 234/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2568 - accuracy: 0.8969\n",
      "Epoch 235/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8969\n",
      "Epoch 236/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8969\n",
      "Epoch 237/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8969\n",
      "Epoch 238/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8969\n",
      "Epoch 239/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8969\n",
      "Epoch 240/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8968\n",
      "Epoch 241/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8968\n",
      "Epoch 242/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8968\n",
      "Epoch 243/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8968\n",
      "Epoch 244/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8968\n",
      "Epoch 245/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2567 - accuracy: 0.8968\n",
      "Epoch 246/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 247/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 248/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 249/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 250/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 251/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8969\n",
      "Epoch 252/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8969\n",
      "Epoch 253/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8969\n",
      "Epoch 254/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 255/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 256/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 257/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 258/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 259/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2565 - accuracy: 0.8968\n",
      "Epoch 260/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2565 - accuracy: 0.8968\n",
      "Epoch 261/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 262/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2565 - accuracy: 0.8968\n",
      "Epoch 263/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2565 - accuracy: 0.8968\n",
      "Epoch 264/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2565 - accuracy: 0.8968\n",
      "Epoch 265/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2565 - accuracy: 0.8968\n",
      "Epoch 266/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 267/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 268/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 269/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 270/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 271/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2565 - accuracy: 0.8969\n",
      "Epoch 272/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 273/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 274/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 275/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 276/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 277/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 278/500\n",
      "902/902 [==============================] - 10s 11ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 279/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 280/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 281/500\n",
      "902/902 [==============================] - 10s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 282/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 283/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 284/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 285/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 286/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2564 - accuracy: 0.8969\n",
      "Epoch 287/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 288/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 289/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 290/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 291/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 292/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 293/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 294/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 295/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 296/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 297/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8969\n",
      "Epoch 298/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8970\n",
      "Epoch 299/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8970\n",
      "Epoch 300/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2563 - accuracy: 0.8970\n",
      "Epoch 301/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2563 - accuracy: 0.8970\n",
      "Epoch 302/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2563 - accuracy: 0.8970\n",
      "Epoch 303/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 304/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 305/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 306/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 307/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 308/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 309/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 310/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 311/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 312/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 313/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 314/500\n",
      "902/902 [==============================] - 15s 16ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 315/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 316/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 317/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 318/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 319/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 320/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 321/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 322/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2561 - accuracy: 0.8970\n",
      "Epoch 323/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 324/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 325/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 326/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 327/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 328/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 329/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 330/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 331/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 332/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 333/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 334/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 335/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 336/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2561 - accuracy: 0.8971\n",
      "Epoch 337/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2561 - accuracy: 0.8972\n",
      "Epoch 338/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2561 - accuracy: 0.8972\n",
      "Epoch 339/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2561 - accuracy: 0.8972\n",
      "Epoch 340/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2561 - accuracy: 0.8972\n",
      "Epoch 341/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 342/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 343/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 344/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 345/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 346/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 347/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 348/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 349/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 350/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 351/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 352/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 353/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 354/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 355/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 356/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 357/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 358/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 359/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 360/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 361/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2560 - accuracy: 0.8972\n",
      "Epoch 362/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 363/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 364/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 365/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 366/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 367/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 368/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 369/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 370/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 371/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 372/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 373/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 374/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 375/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 376/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 377/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 378/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 379/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2559 - accuracy: 0.8971\n",
      "Epoch 380/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 381/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 382/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 383/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 384/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 385/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2559 - accuracy: 0.8972\n",
      "Epoch 386/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 387/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 388/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 389/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 390/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 391/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 392/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 393/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 394/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 395/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 396/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 397/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 398/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 399/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 400/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 401/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 402/500\n",
      "902/902 [==============================] - 11s 12ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 403/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 404/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 405/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 406/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8973\n",
      "Epoch 407/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 408/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 409/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 410/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 411/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 412/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 413/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 414/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 415/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 416/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2558 - accuracy: 0.8972\n",
      "Epoch 417/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 418/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 419/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 420/500\n",
      "902/902 [==============================] - 15s 17ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 421/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 422/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 423/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 424/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 425/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 426/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 427/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 428/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 429/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 430/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 431/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 432/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 433/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 434/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 435/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 436/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2557 - accuracy: 0.8972\n",
      "Epoch 437/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 438/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 439/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 440/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 441/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 442/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 443/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 444/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 445/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 446/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 447/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 448/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 449/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 450/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 451/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 452/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 453/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 454/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2557 - accuracy: 0.8973\n",
      "Epoch 455/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 456/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 457/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 458/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 459/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 460/500\n",
      "902/902 [==============================] - 16s 17ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 461/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 462/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 463/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 464/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 465/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 466/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 467/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 468/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 469/500\n",
      "902/902 [==============================] - 14s 15ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 470/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 471/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 472/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 473/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 474/500\n",
      "902/902 [==============================] - 12s 14ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 475/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 476/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 477/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 478/500\n",
      "902/902 [==============================] - 11s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 479/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 480/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 481/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 482/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 483/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 484/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 485/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 486/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 487/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 488/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 489/500\n",
      "902/902 [==============================] - 14s 16ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 490/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 491/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 492/500\n",
      "902/902 [==============================] - 13s 15ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 493/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 494/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 495/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 496/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 497/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8974\n",
      "Epoch 498/500\n",
      "902/902 [==============================] - 12s 13ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 499/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n",
      "Epoch 500/500\n",
      "902/902 [==============================] - 13s 14ms/step - loss: 0.2556 - accuracy: 0.8973\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c97496d130>"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "\r\n",
    "vocab=list(worddict.keys())\r\n",
    "vocab.insert(0,\"<PAD>\")\r\n",
    "len(vocab)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5228"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "import io\r\n",
    "import re\r\n",
    "import string\r\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\r\n",
    "\r\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\r\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\r\n",
    "\r\n",
    "for index, word in enumerate(vocab):\r\n",
    "  if index == 0:\r\n",
    "    continue  # skip 0, it's padding.\r\n",
    "  vec = weights[index]\r\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\r\n",
    "  out_m.write(word + \"\\n\")\r\n",
    "out_v.close()\r\n",
    "out_m.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "weights.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5228, 128)"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "vocab_weights={}\r\n",
    "weightslist=weights.tolist()\r\n",
    "for index,word in enumerate(vocab):\r\n",
    "    if index==0:\r\n",
    "        continue\r\n",
    "    vocab_weights[word]=weightslist[index]\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/new_vocabs_weights.json\",\"w\",encoding=\"utf-8\") as f:\r\n",
    "    json.dump(vocab_weights,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "vocab=list(worddict.keys())\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\r\n",
    "weights[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.02234798, -0.02036996,  0.0244733 , -0.006604  , -0.01843094,\n",
       "       -0.00436572, -0.03649978,  0.04763223,  0.03983975,  0.01366086,\n",
       "       -0.04861617,  0.0159493 ,  0.03891993,  0.01603014,  0.01695739,\n",
       "        0.02715579,  0.01401539,  0.02352457, -0.03075529, -0.00376249,\n",
       "        0.01570125, -0.03463662,  0.01790473,  0.0247595 , -0.04686971,\n",
       "       -0.03932341, -0.02829659, -0.01633142,  0.0465782 , -0.04074208,\n",
       "       -0.02168462,  0.03254963,  0.03429175, -0.02250403,  0.02632345,\n",
       "       -0.04833278, -0.00497564,  0.0046381 , -0.03242748,  0.02242507,\n",
       "       -0.00606018, -0.00605351, -0.04198269, -0.04717908, -0.00924442,\n",
       "       -0.0255707 ,  0.0108913 ,  0.04028672, -0.03128753, -0.00847763,\n",
       "        0.02337762,  0.04639346, -0.00500913,  0.04189097,  0.03297232,\n",
       "       -0.04273205,  0.02790445, -0.04894259,  0.04627626,  0.01257626,\n",
       "       -0.0080957 ,  0.02868903, -0.04154355,  0.04719685,  0.0476823 ,\n",
       "        0.04542429, -0.00266172,  0.04211965,  0.01761994, -0.01312793,\n",
       "       -0.04500693, -0.02897017, -0.03167085, -0.02993706,  0.02669844,\n",
       "        0.01934335,  0.00757754, -0.04258701,  0.01662247,  0.02170699,\n",
       "        0.00924288,  0.02743225, -0.02239598, -0.04156778, -0.00733451,\n",
       "       -0.01767228, -0.00844867,  0.03080059,  0.01111675, -0.02556809,\n",
       "       -0.00774004, -0.01374482,  0.00354797,  0.03380312,  0.03985239,\n",
       "        0.02034544, -0.03367118,  0.0262859 ,  0.01904073,  0.02750072,\n",
       "       -0.00878615,  0.02875222,  0.01732552,  0.00341889, -0.01635107,\n",
       "        0.01614941, -0.01247375,  0.03267156,  0.03545607,  0.0380476 ,\n",
       "        0.04630882,  0.02035543, -0.03606731,  0.01432382,  0.02174758,\n",
       "       -0.00610822, -0.02209603, -0.04671937, -0.00111701,  0.02831898,\n",
       "       -0.0443071 ,  0.01130061, -0.03645673,  0.00503551, -0.03727697,\n",
       "        0.00876981, -0.01707097, -0.03391559], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "def cosine_simalarity(label,target):\r\n",
    "    return np.dot(label,target)/(np.dot(np.linalg.norm(label),np.linalg.norm(target)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "chosenword=input()\r\n",
    "if vocab.count(chosenword)>0:\r\n",
    "    print(chosenword)\r\n",
    "    for i,word in enumerate(vocab):\r\n",
    "        if word==chosenword:\r\n",
    "            chosenweight=weights[i]\r\n",
    "    cosinelist={}\r\n",
    "    for i,word in enumerate(vocab):\r\n",
    "        if word!=chosenword:\r\n",
    "            cosinelist[word]=cosine_simalarity(chosenweight,weights[i])\r\n",
    "    import pandas as pd\r\n",
    "    dataframe=pd.DataFrame({\"Words\":cosinelist.keys(),\"Similarity\":cosinelist.values()})\r\n",
    "    print(dataframe.sort_values(by=[\"Similarity\"],ascending=False).head(10))\r\n",
    "else:\r\n",
    "    print(\"word not found\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "đúc kết\n",
      "           Words  Similarity\n",
      "944    quy trình    0.857267\n",
      "566        chỉnh    0.792563\n",
      "943      quá khứ    0.754430\n",
      "968      số phức    0.675230\n",
      "942  kinh nghiệm    0.674900\n",
      "766     những ai    0.672484\n",
      "859      nhu cầu    0.658019\n",
      "932    lập trình    0.646847\n",
      "438         phức    0.639821\n",
      "668     ứng dụng    0.628425\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}