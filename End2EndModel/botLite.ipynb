{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/intentsVNver1.json\",encoding=\"utf-8\") as file:\r\n",
    "    data=json.load(file)\r\n",
    "patternlist=[]\r\n",
    "labels=[]\r\n",
    "for dt in data[\"intents\"]:\r\n",
    "    for pattern in dt[\"patterns\"]:\r\n",
    "        patternlist.append(pattern)\r\n",
    "        labels.append(dt[\"tag\"])\r\n",
    "labels\r\n",
    "labels=np.array(labels)\r\n",
    "labelsOH=pd.get_dummies(labels)\r\n",
    "labellist=list(labelsOH)\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/bi_grams.txt\",encoding=\"utf-8\") as f:\r\n",
    "    bi_gramtxt=f.read()\r\n",
    "bi_gram_list=[]\r\n",
    "for each in bi_gramtxt.split(\",\"):\r\n",
    "    each=each[:-1]\r\n",
    "    each=each[2:]\r\n",
    "    each=each.replace(\" \",\"_\")\r\n",
    "    bi_gram_list.append(each)\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/vietnamese-stopwords.txt\",encoding=\"utf-8\") as f:\r\n",
    "    stopword_txt=f.readlines()\r\n",
    "    \r\n",
    "stopword_list=[]\r\n",
    "for each in stopword_txt:\r\n",
    "    stopword_list.append(each[:-1])\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/posdata.json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "    mpos=json.load(f)\r\n",
    "\r\n",
    "responses_dict={}\r\n",
    "for i in data[\"intents\"]:\r\n",
    "    responses_dict[i[\"tag\"]]=i[\"responses\"]\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/knowledge.json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "    knowledge=json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "list(knowledge.values())[0][\"pizza\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "list(knowledge.values())[0]\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'pizza': {'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'},\n",
       " 'phở': {'favorite': 'yes',\n",
       "  'info': ['ngon', 'ngon hơn nữa'],\n",
       "  'xuất xứ': 'Việt Nam'},\n",
       " 'đậu xanh': {'favorite': 'no', 'info': ['không ngon']}}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "class brain_exe():\r\n",
    "    def __init__(self,knowledge):\r\n",
    "        self.mem={}\r\n",
    "        self.dialogue_count=0\r\n",
    "        self.length_limit=20\r\n",
    "        self.knowledge_base=knowledge\r\n",
    "\r\n",
    "    def pos_filter(self,pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\r\n",
    "        pos_filtered=[]\r\n",
    "        for i in pos_rs:\r\n",
    "            if (i[1]==pos_name and stopword.count(i[0])==0):\r\n",
    "                pos_filtered.append(i)\r\n",
    "        return pos_filtered\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.mem={}\r\n",
    "    \r\n",
    "    def add(self,intent,sentence_entities):\r\n",
    "        self.mem[self.dialogue_count]=[intent,sentence_entities]\r\n",
    "        self.dialogue_count=self.dialogue_count+1\r\n",
    "        if self.dialogue_count==self.length_limit:\r\n",
    "            self.mem={}\r\n",
    "\r\n",
    "    def lookup(self):\r\n",
    "        return self.mem\r\n",
    "\r\n",
    "    def knowledge_lookup(self,entities):\r\n",
    "        if len(entities)!=0:\r\n",
    "            verb_list=self.pos_filter(entities,\"verb\")\r\n",
    "            noun_list=self.pos_filter(entities,\"noun\")\r\n",
    "            result=[]\r\n",
    "            for i in noun_list:\r\n",
    "                i[0]=i[0].replace(\"_\",\" \")\r\n",
    "                for each in list(knowledge.values()):\r\n",
    "                    for j in each.keys():\r\n",
    "                        if i[0]==j:\r\n",
    "                            result.append([j,each[j]])\r\n",
    "            return result\r\n",
    "        else:\r\n",
    "            return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class FTokenizer():\r\n",
    "    def __init__(self,bi_gram_list):\r\n",
    "        self.bi_gram_list=bi_gram_list\r\n",
    "\r\n",
    "    def bi_gram_checker(self,bi_gram_word):\r\n",
    "        for i in self.bi_gram_list:\r\n",
    "            if (bi_gram_word==i):\r\n",
    "                return True\r\n",
    "        return False\r\n",
    "    \r\n",
    "    def fit_on_text(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (myContinue):\r\n",
    "                    myContinue=False\r\n",
    "                    continue\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        myContinue=True\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (myContinue==False):  \r\n",
    "                    if (tokens.count(listtokens[i])==0):\r\n",
    "                        tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "\r\n",
    "    def fit_on_text2(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (tokens.count(listtokens[i])==0 and listtokens[i]!=\"\"):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "    \r\n",
    "    def get_word_to_index(self):\r\n",
    "        return self.word_to_index\r\n",
    "    \r\n",
    "    def get_index_to_word(self):\r\n",
    "        return self.index_to_word\r\n",
    "\r\n",
    "    def get_max_length(self,data):\r\n",
    "        maxsen=0\r\n",
    "        for i in data:\r\n",
    "            if maxsen<len(i):\r\n",
    "                maxsen=len(i)\r\n",
    "        return maxsen\r\n",
    "\r\n",
    "    def text_to_sequence(self,textdata):\r\n",
    "        d_continue=False\r\n",
    "        output=[]\r\n",
    "        for text in textdata:\r\n",
    "            text=text.lower().split(\" \")\r\n",
    "            text.append(\"\")\r\n",
    "            sth=[]\r\n",
    "            d_continue=False\r\n",
    "            for i in range(0,len(text)-1):\r\n",
    "                if d_continue==True:\r\n",
    "                    d_continue=False\r\n",
    "                    continue\r\n",
    "                if (text[i+1]!=\"\"):\r\n",
    "                    pairtext=text[i]+\"_\"+text[i+1]\r\n",
    "                    for j in self.word_to_index:\r\n",
    "                        if pairtext.lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            d_continue=True\r\n",
    "                            break\r\n",
    "                if d_continue==False:\r\n",
    "                    for count,j in enumerate(self.word_to_index):\r\n",
    "                        if text[i].lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            break\r\n",
    "                        if count==len(self.word_to_index)-1:\r\n",
    "                            sth.append(1)\r\n",
    "            output.append(sth)\r\n",
    "        return output\r\n",
    "    \r\n",
    "    def intseq_to_index(self,sequence):\r\n",
    "        output=[]\r\n",
    "        for i in sequence:\r\n",
    "            output.append(self.index_to_word[i])\r\n",
    "        return output\r\n",
    "\r\n",
    "    def pad_sequence(self,datasequence,maxlen):\r\n",
    "        for i in datasequence:\r\n",
    "            while len(i)!=maxlen:\r\n",
    "                i.insert(0,0)\r\n",
    "        return datasequence\r\n",
    "\r\n",
    "    def bi_gram_ize(self,text):\r\n",
    "        listtokens=text.lower().split(\" \")\r\n",
    "        listtokens.append(\"\")\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for i in range(0,len(listtokens)-1):\r\n",
    "            if (myContinue):\r\n",
    "                myContinue=False\r\n",
    "                continue\r\n",
    "            if (listtokens[i+1]!=\"\"):\r\n",
    "                pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                if (self.bi_gram_checker(pairtext)):\r\n",
    "                    myContinue=True\r\n",
    "                    if (tokens.count(pairtext)==0):\r\n",
    "                        tokens.append(pairtext)\r\n",
    "            if (myContinue==False):  \r\n",
    "                if (tokens.count(listtokens[i])==0):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "        return tokens\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "myTokenizer=FTokenizer(bi_gram_list)\r\n",
    "myTokenizer.fit_on_text2(patternlist)\r\n",
    "pattern_tokenized2=myTokenizer.text_to_sequence(patternlist)\r\n",
    "pattern_preprocessed2=tf.keras.preprocessing.sequence.pad_sequences(pattern_tokenized2)\r\n",
    "mlen2=pattern_preprocessed2.shape[1]\r\n",
    "\r\n",
    "posmesTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token=\"<UNK>\")\r\n",
    "posmesTokenizer.fit_on_texts(mpos[\"messages\"])\r\n",
    "mes_Tokenized=posmesTokenizer.texts_to_sequences(mpos[\"messages\"])\r\n",
    "finalmes=tf.keras.preprocessing.sequence.pad_sequences(mes_Tokenized)\r\n",
    "poslen=finalmes.shape[1]\r\n",
    "\r\n",
    "postagTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000)\r\n",
    "postagTokenizer.fit_on_texts(mpos[\"pos\"])\r\n",
    "pos_tokenized=postagTokenizer.texts_to_sequences(mpos[\"pos\"])\r\n",
    "finalpos=tf.keras.preprocessing.sequence.pad_sequences(pos_tokenized)\r\n",
    "postagTokenizer.index_word[0]=\"<PAD>\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "class BotBackendLite():\r\n",
    "    def __init__(self,posdata,intentdata,bi_gram_list,labellist):\r\n",
    "        self.posdata=posdata\r\n",
    "        self.intent_data=intentdata\r\n",
    "        self.bi_gram_list=bi_gram_list\r\n",
    "        self.labellist=np.array(labellist+[\"NULL\"])\r\n",
    "        self.response_dictionary=responses_dict\r\n",
    "        self.response_dictionary[\"NULL\"]=[\"Mình không hiểu lắm\",\"Là sao vậy ? Mình không hiểu cậu lắm\",\"Là gì thế ?\"]\r\n",
    "\r\n",
    "        self.interpreter=tf.lite.Interpreter(\"E:\\MLCourse/foxBot/foxPython/End2EndModel/model/tfliteModel/model.tflite\")\r\n",
    "        self.interpreter.allocate_tensors()\r\n",
    "        self.input_details=self.interpreter.get_input_details()\r\n",
    "        self.output_details=self.interpreter.get_output_details()\r\n",
    "        self.input_shape1=self.input_details[0][\"shape\"]\r\n",
    "        self.input_shape2=self.input_details[1][\"shape\"]\r\n",
    "        self.brain=brain_exe(knowledge)\r\n",
    "\r\n",
    "    def preprocess(self,inp):\r\n",
    "        inp_tokenized=myTokenizer.text_to_sequence([inp])\r\n",
    "        inp_preprocessed=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=mlen2)\r\n",
    "\r\n",
    "        inp_gramized=myTokenizer.bi_gram_ize(inp)\r\n",
    "        inp_tokenized=posmesTokenizer.texts_to_sequences([inp_gramized])\r\n",
    "        final_inp=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=finalmes.shape[1])\r\n",
    "        return inp_preprocessed.astype(np.int64),final_inp.astype(np.int64)\r\n",
    "\r\n",
    "    def posAnalyze(self,inp,posdata):\r\n",
    "        pos=[]\r\n",
    "        for i in posdata[0]:\r\n",
    "            if postagTokenizer.index_word[np.argmax(i)]!=\"<PAD>\":\r\n",
    "                pos.append(postagTokenizer.index_word[np.argmax(i)])\r\n",
    "        l=[]\r\n",
    "        for i in range(0,len(pos)):\r\n",
    "            if pos[i]!=\"<PAD>\":\r\n",
    "                l.append([inp[i],pos[i]])\r\n",
    "        return l\r\n",
    "\r\n",
    "    def pos_filter(self,pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\r\n",
    "        pos_filtered=[]\r\n",
    "        for i in pos_rs:\r\n",
    "            if (i[1]==pos_name and stopword.count(i[0])==0):\r\n",
    "                pos_filtered.append(i)\r\n",
    "        return pos_filtered\r\n",
    "\r\n",
    "    def accept_accuracy(self,pred,acc):\r\n",
    "        if (max(pred[0])>acc):\r\n",
    "            return np.argmax(pred[0])\r\n",
    "        else:\r\n",
    "            return len(pred[0])\r\n",
    "\r\n",
    "    def debugMode(self,printlist):\r\n",
    "        for count,i in enumerate(printlist):\r\n",
    "            print(count,\"/\",i)\r\n",
    "\r\n",
    "    def responses_generate(self,inp):\r\n",
    "        return random.choice(self.response_dictionary[inp])\r\n",
    "\r\n",
    "    def bot_action(self,inp,entities,brain_dict=False):\r\n",
    "        if inp==\"question_from_user\":\r\n",
    "            if len(brain_dict)!=0:\r\n",
    "                try:\r\n",
    "                    return brain_dict[0][1][\"define\"]\r\n",
    "                except:\r\n",
    "                    return \"Nó khó tả lắm sorry\"\r\n",
    "            return \"Sorry, hiện tại mình chưa biết nữa\"\r\n",
    "\r\n",
    "        if inp==\"askingme\":\r\n",
    "            verb_list=self.pos_filter(entities,\"verb\")\r\n",
    "            noun_list=self.pos_filter(entities,\"noun\")\r\n",
    "            if len(brain_dict)!=0:\r\n",
    "                return random.choice([\"Có\",\"Yess\",\"Yes\"])\r\n",
    "            else:\r\n",
    "                return random.choice([\"Không\",\"No\",\"Nah\"])\r\n",
    "        \r\n",
    "        if inp!=\"askingme\" and inp!=\"question_from_user\":\r\n",
    "            return self.responses_generate(inp)\r\n",
    "  \r\n",
    "    def chat(self):\r\n",
    "        debugToggle=False\r\n",
    "        while True:\r\n",
    "            inp=input(\"You: \")\r\n",
    "            # Hotkey\r\n",
    "            if inp==\"quit\":\r\n",
    "                break\r\n",
    "            if inp==\"!debugmode1\":\r\n",
    "                print(\"DEV: DEBUG ON\")\r\n",
    "                debugToggle=True\r\n",
    "                continue\r\n",
    "            if inp==\"!debugmode0\":\r\n",
    "                print(\"DEV: DEBUG OFF\")\r\n",
    "                debugToggle=False\r\n",
    "                continue\r\n",
    "    \r\n",
    "            intent,pos=self.preprocess(inp)\r\n",
    "\r\n",
    "            self.interpreter.set_tensor(self.input_details[0][\"index\"],pos)\r\n",
    "            self.interpreter.set_tensor(self.input_details[1][\"index\"],intent)\r\n",
    "            self.interpreter.invoke()\r\n",
    "            pos_result = self.interpreter.get_tensor(self.output_details[0]['index'])\r\n",
    "            intent_result = self.interpreter.get_tensor(self.output_details[1]['index'])\r\n",
    "            # rs=myModel.predict([intent,pos])\r\n",
    "            pos_rs=self.posAnalyze(myTokenizer.bi_gram_ize(inp),pos_result)\r\n",
    "            brain_dict=self.brain.knowledge_lookup(pos_rs)\r\n",
    "            intent_output=self.labellist[self.accept_accuracy(intent_result,0.8)]\r\n",
    "\r\n",
    "            print(\"You: \",inp)\r\n",
    "            if debugToggle==True:\r\n",
    "                self.debugMode([\r\n",
    "                    [\"POS: \",pos_rs],\r\n",
    "                    [\"Intent: \",intent_output],\r\n",
    "                    [\"Acc: \",max(intent_result[0])],\r\n",
    "                    [\"Bot: \",self.bot_action(intent_output,pos_rs,brain_dict)],\r\n",
    "                    [\"Brain Check: \",brain_dict]\r\n",
    "                    ])\r\n",
    "            else:\r\n",
    "                print(\"Bot: \",self.bot_action(intent_output,pos_rs,brain_dict))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "botBackend=BotBackendLite(mpos,data,bi_gram_list,labellist)\r\n",
    "botBackend.chat()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You:  Hôm nay sao rồi\n",
      "Bot:  Ổn ổn\n",
      "You:  Giúp mình bài nay nha\n",
      "Bot:  Mình biết vấn đề của bạn được không ?\n",
      "You:  vật lý đại cương\n",
      "Bot:  Goodbye!\n",
      "You:  cậu thích đọc sách không\n",
      "Bot:  Sorry, hiện tại mình chưa biết nữa\n",
      "DEV: DEBUG ON\n",
      "You:  cậu thích đọc sách không\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['đọc', 'noun'], ['sách', 'x']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9998431]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích sách không\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['sách', 'noun'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.99969935]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu có thích món ăn nào không\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['có', 'verb'], ['thích', 'verb'], ['món', 'noun'], ['ăn', 'verb'], ['nào', 'propn'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.80592346]\n",
      "3 / ['Bot: ', 'Không']\n",
      "4 / ['Brain Check: ', []]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "3d95097792c4c1f8621d6eabaad3727eb6f403675c9e5393220cb43082419a68"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}