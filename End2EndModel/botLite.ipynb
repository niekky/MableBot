{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/intentsVNver1.json\",encoding=\"utf-8\") as file:\n",
    "    data=json.load(file)\n",
    "patternlist=[]\n",
    "labels=[]\n",
    "for dt in data[\"intents\"]:\n",
    "    for pattern in dt[\"patterns\"]:\n",
    "        patternlist.append(pattern)\n",
    "        labels.append(dt[\"tag\"])\n",
    "labels\n",
    "labels=np.array(labels)\n",
    "labelsOH=pd.get_dummies(labels)\n",
    "labellist=list(labelsOH)\n",
    "\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/bi_grams.txt\",encoding=\"utf-8\") as f:\n",
    "    bi_gramtxt=f.read()\n",
    "bi_gram_list=[]\n",
    "for each in bi_gramtxt.split(\",\"):\n",
    "    each=each[:-1]\n",
    "    each=each[2:]\n",
    "    each=each.replace(\" \",\"_\")\n",
    "    bi_gram_list.append(each)\n",
    "\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/vietnamese-stopwords.txt\",encoding=\"utf-8\") as f:\n",
    "    stopword_txt=f.readlines()\n",
    "    \n",
    "stopword_list=[]\n",
    "for each in stopword_txt:\n",
    "    stopword_list.append(each[:-1])\n",
    "\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/posdata.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    mpos=json.load(f)\n",
    "\n",
    "responses_dict={}\n",
    "for i in data[\"intents\"]:\n",
    "    responses_dict[i[\"tag\"]]=i[\"responses\"]\n",
    "\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/knowledge.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    knowledge=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(knowledge.values())[0][\"pizza\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pizza': {'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'},\n",
       " 'phở': {'favorite': 'yes',\n",
       "  'info': ['ngon', 'ngon hơn nữa'],\n",
       "  'xuất xứ': 'Việt Nam'},\n",
       " 'đậu xanh': {'favorite': 'no', 'info': ['không ngon']}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(knowledge.values())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class brain_exe():\n",
    "    def __init__(self,knowledge):\n",
    "        self.mem={}\n",
    "        self.dialogue_count=0\n",
    "        self.length_limit=20\n",
    "        self.knowledge_base=knowledge\n",
    "\n",
    "    def pos_filter(self,pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\n",
    "        pos_filtered=[]\n",
    "        for i in pos_rs:\n",
    "            if (i[1]==pos_name and stopword.count(i[0])==0):\n",
    "                pos_filtered.append(i)\n",
    "        return pos_filtered\n",
    "\n",
    "    def reset(self):\n",
    "        self.mem={}\n",
    "    \n",
    "    def add(self,intent,sentence_entities):\n",
    "        self.mem[self.dialogue_count]=[intent,sentence_entities]\n",
    "        self.dialogue_count=self.dialogue_count+1\n",
    "        if self.dialogue_count==self.length_limit:\n",
    "            self.mem={}\n",
    "\n",
    "    def lookup(self):\n",
    "        return self.mem\n",
    "\n",
    "    def knowledge_lookup(self,entities):\n",
    "        if len(entities)!=0:\n",
    "            verb_list=self.pos_filter(entities,\"verb\")\n",
    "            noun_list=self.pos_filter(entities,\"noun\")\n",
    "            result=[]\n",
    "            for i in noun_list:\n",
    "                i[0]=i[0].replace(\"_\",\" \")\n",
    "                for each in list(knowledge.values()):\n",
    "                    for j in each.keys():\n",
    "                        if i[0]==j:\n",
    "                            result.append([j,each[j]])\n",
    "            return result\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTokenizer():\n",
    "    def __init__(self,bi_gram_list):\n",
    "        self.bi_gram_list=bi_gram_list\n",
    "\n",
    "    def bi_gram_checker(self,bi_gram_word):\n",
    "        for i in self.bi_gram_list:\n",
    "            if (bi_gram_word==i):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def fit_on_text(self,data):\n",
    "        word_to_index={}\n",
    "        index_to_word={}\n",
    "        word_to_index[\"<PAD>\"]=0\n",
    "        word_to_index[\"<OOV>\"]=1\n",
    "        index_to_word[0]=\"<PAD>\"\n",
    "        index_to_word[1]=\"<OOV>\"\n",
    "        myContinue=False\n",
    "        tokens=[]\n",
    "        for eachseq in data:\n",
    "            listtokens=eachseq.lower().split(\" \")\n",
    "            listtokens.append(\"\")\n",
    "            for i in range(0,len(listtokens)-1):\n",
    "                if (myContinue):\n",
    "                    myContinue=False\n",
    "                    continue\n",
    "                if (listtokens[i+1]!=\"\"):\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\n",
    "                    if (self.bi_gram_checker(pairtext)):\n",
    "                        myContinue=True\n",
    "                        if (tokens.count(pairtext)==0):\n",
    "                            tokens.append(pairtext)\n",
    "                if (myContinue==False):  \n",
    "                    if (tokens.count(listtokens[i])==0):\n",
    "                        tokens.append(listtokens[i])\n",
    "        for index,word in enumerate(tokens):\n",
    "            word_to_index[word]=index+2\n",
    "            index_to_word[index+2]=word\n",
    "        self.word_to_index=word_to_index\n",
    "        self.index_to_word=index_to_word\n",
    "\n",
    "    def fit_on_text2(self,data):\n",
    "        word_to_index={}\n",
    "        index_to_word={}\n",
    "        word_to_index[\"<PAD>\"]=0\n",
    "        word_to_index[\"<OOV>\"]=1\n",
    "        index_to_word[0]=\"<PAD>\"\n",
    "        index_to_word[1]=\"<OOV>\"\n",
    "        myContinue=False\n",
    "        tokens=[]\n",
    "        for eachseq in data:\n",
    "            listtokens=eachseq.lower().split(\" \")\n",
    "            listtokens.append(\"\")\n",
    "            for i in range(0,len(listtokens)-1):\n",
    "                if (listtokens[i+1]!=\"\"):\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\n",
    "                    if (self.bi_gram_checker(pairtext)):\n",
    "                        if (tokens.count(pairtext)==0):\n",
    "                            tokens.append(pairtext)\n",
    "                if (tokens.count(listtokens[i])==0 and listtokens[i]!=\"\"):\n",
    "                    tokens.append(listtokens[i])\n",
    "        for index,word in enumerate(tokens):\n",
    "            word_to_index[word]=index+2\n",
    "            index_to_word[index+2]=word\n",
    "        self.word_to_index=word_to_index\n",
    "        self.index_to_word=index_to_word\n",
    "    \n",
    "    def get_word_to_index(self):\n",
    "        return self.word_to_index\n",
    "    \n",
    "    def get_index_to_word(self):\n",
    "        return self.index_to_word\n",
    "\n",
    "    def get_max_length(self,data):\n",
    "        maxsen=0\n",
    "        for i in data:\n",
    "            if maxsen<len(i):\n",
    "                maxsen=len(i)\n",
    "        return maxsen\n",
    "\n",
    "    def text_to_sequence(self,textdata):\n",
    "        d_continue=False\n",
    "        output=[]\n",
    "        for text in textdata:\n",
    "            text=text.lower().split(\" \")\n",
    "            text.append(\"\")\n",
    "            sth=[]\n",
    "            d_continue=False\n",
    "            for i in range(0,len(text)-1):\n",
    "                if d_continue==True:\n",
    "                    d_continue=False\n",
    "                    continue\n",
    "                if (text[i+1]!=\"\"):\n",
    "                    pairtext=text[i]+\"_\"+text[i+1]\n",
    "                    for j in self.word_to_index:\n",
    "                        if pairtext.lower()==j:\n",
    "                            sth.append(self.word_to_index[j])\n",
    "                            d_continue=True\n",
    "                            break\n",
    "                if d_continue==False:\n",
    "                    for count,j in enumerate(self.word_to_index):\n",
    "                        if text[i].lower()==j:\n",
    "                            sth.append(self.word_to_index[j])\n",
    "                            break\n",
    "                        if count==len(self.word_to_index)-1:\n",
    "                            sth.append(1)\n",
    "            output.append(sth)\n",
    "        return output\n",
    "    \n",
    "    def intseq_to_index(self,sequence):\n",
    "        output=[]\n",
    "        for i in sequence:\n",
    "            output.append(self.index_to_word[i])\n",
    "        return output\n",
    "\n",
    "    def pad_sequence(self,datasequence,maxlen):\n",
    "        for i in datasequence:\n",
    "            while len(i)!=maxlen:\n",
    "                i.insert(0,0)\n",
    "        return datasequence\n",
    "\n",
    "    def bi_gram_ize(self,text):\n",
    "        listtokens=text.lower().split(\" \")\n",
    "        listtokens.append(\"\")\n",
    "        myContinue=False\n",
    "        tokens=[]   \n",
    "        for i in range(0,len(listtokens)-1):\n",
    "            if (myContinue):\n",
    "                myContinue=False\n",
    "                continue\n",
    "            if (listtokens[i+1]!=\"\"):\n",
    "                pairtext=listtokens[i]+\"_\"+listtokens[i+1]\n",
    "                if (self.bi_gram_checker(pairtext)):\n",
    "                    myContinue=True\n",
    "                    tokens.append(pairtext)\n",
    "            if (myContinue==False):  \n",
    "                tokens.append(listtokens[i])\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTokenizer=FTokenizer(bi_gram_list)\n",
    "myTokenizer.fit_on_text2(patternlist)\n",
    "pattern_tokenized2=myTokenizer.text_to_sequence(patternlist)\n",
    "pattern_preprocessed2=tf.keras.preprocessing.sequence.pad_sequences(pattern_tokenized2)\n",
    "mlen2=pattern_preprocessed2.shape[1]\n",
    "\n",
    "posmesTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token=\"<UNK>\")\n",
    "posmesTokenizer.fit_on_texts(mpos[\"messages\"])\n",
    "mes_Tokenized=posmesTokenizer.texts_to_sequences(mpos[\"messages\"])\n",
    "finalmes=tf.keras.preprocessing.sequence.pad_sequences(mes_Tokenized)\n",
    "poslen=finalmes.shape[1]\n",
    "\n",
    "postagTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
    "postagTokenizer.fit_on_texts(mpos[\"pos\"])\n",
    "pos_tokenized=postagTokenizer.texts_to_sequences(mpos[\"pos\"])\n",
    "finalpos=tf.keras.preprocessing.sequence.pad_sequences(pos_tokenized)\n",
    "postagTokenizer.index_word[0]=\"<PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "intentdict=myTokenizer.word_to_index\n",
    "posmesdict=posmesTokenizer.word_index\n",
    "postagdict=postagTokenizer.word_index\n",
    "this_bi_gram_lict=bi_gram_list\n",
    "posmesdict[\"<PAD>\"]=0\n",
    "postagdict[\"<PAD>\"]=0\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/intentDict.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(intentdict,f)\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/posmesDict.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(posmesdict,f)\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/postagDict.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(postagdict,f)\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/this_bg_list.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(this_bi_gram_lict,f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BotBackendLite():\n",
    "    def __init__(self,posdata,intentdata,bi_gram_list,labellist):\n",
    "        self.posdata=posdata\n",
    "        self.intent_data=intentdata\n",
    "        self.bi_gram_list=bi_gram_list\n",
    "        self.labellist=np.array(labellist+[\"NULL\"])\n",
    "        self.response_dictionary=responses_dict\n",
    "        self.response_dictionary[\"NULL\"]=[\"Mình không hiểu lắm\",\"Là sao vậy ? Mình không hiểu cậu lắm\",\"Là gì thế ?\"]\n",
    "\n",
    "        self.interpreter=tf.lite.Interpreter(\"E:/MLCourse/foxBot/foxPython/End2EndModel/model/tfliteModel/model3.tflite\")\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details=self.interpreter.get_input_details()\n",
    "        self.output_details=self.interpreter.get_output_details()\n",
    "        self.input_shape1=self.input_details[0][\"shape\"]\n",
    "        self.input_shape2=self.input_details[1][\"shape\"]\n",
    "        self.brain=brain_exe(knowledge)\n",
    "\n",
    "    def preprocess(self,inp):\n",
    "        punc_list=[\",\",\".\",\"'\",\"[\",\"]\",\"!\",\"?\"]\n",
    "        for i in punc_list:\n",
    "            inp=inp.replace(i,\"\")\n",
    "        inp_tokenized=myTokenizer.text_to_sequence([inp])\n",
    "        inp_preprocessed=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=mlen2)\n",
    "\n",
    "        inp_gramized=myTokenizer.bi_gram_ize(inp)\n",
    "        inp_tokenized=posmesTokenizer.texts_to_sequences([inp_gramized])\n",
    "        final_inp=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=finalmes.shape[1])\n",
    "        return inp_preprocessed,final_inp\n",
    "\n",
    "    def posAnalyze(self,inp,posdata):\n",
    "        pos=[]\n",
    "        for i in posdata[0]:\n",
    "            if postagTokenizer.index_word[np.argmax(i)]!=\"<PAD>\":\n",
    "                pos.append(postagTokenizer.index_word[np.argmax(i)])\n",
    "        l=[]\n",
    "        for i in range(0,len(pos)):\n",
    "            if pos[i]!=\"<PAD>\":\n",
    "                l.append([inp[i],pos[i]])\n",
    "        return l\n",
    "\n",
    "    def pos_filter(self,pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\n",
    "        pos_filtered=[]\n",
    "        for i in pos_rs:\n",
    "            if (i[1]==pos_name and stopword.count(i[0])==0):\n",
    "                pos_filtered.append(i)\n",
    "        return pos_filtered\n",
    "\n",
    "    def accept_accuracy(self,pred,acc):\n",
    "        if (max(pred[0])>acc):\n",
    "            return np.argmax(pred[0])\n",
    "        else:\n",
    "            return len(pred[0])\n",
    "\n",
    "    def debugMode(self,printlist):\n",
    "        for count,i in enumerate(printlist):\n",
    "            print(count,\"/\",i)\n",
    "\n",
    "    def responses_generate(self,inp):\n",
    "        return random.choice(self.response_dictionary[inp])\n",
    "\n",
    "    def bot_action(self,inp,entities,brain_dict=False):\n",
    "        if inp==\"question_from_user\":\n",
    "            if len(brain_dict)!=0:\n",
    "                try:\n",
    "                    return brain_dict[0][1][\"define\"]\n",
    "                except:\n",
    "                    return \"Nó khó tả lắm sorry\"\n",
    "            return \"Sorry, hiện tại mình chưa biết nữa\"\n",
    "\n",
    "        if inp==\"askingme\":\n",
    "            verb_list=self.pos_filter(entities,\"verb\")\n",
    "            noun_list=self.pos_filter(entities,\"noun\")\n",
    "            if len(brain_dict)!=0:\n",
    "                return random.choice([\"Có\",\"Yess\",\"Yes\"])\n",
    "            else:\n",
    "                return random.choice([\"Không\",\"No\",\"Nah\"])\n",
    "        \n",
    "        if inp!=\"askingme\" and inp!=\"question_from_user\":\n",
    "            return self.responses_generate(inp)\n",
    "  \n",
    "    def chat(self):\n",
    "        debugToggle=False\n",
    "        while True:\n",
    "            inp=input(\"You: \")\n",
    "            # Hotkey\n",
    "            if inp==\"quit\":\n",
    "                break\n",
    "            if inp==\"!debugmode1\":\n",
    "                print(\"DEV: DEBUG ON\")\n",
    "                debugToggle=True\n",
    "                continue\n",
    "            if inp==\"!debugmode0\":\n",
    "                print(\"DEV: DEBUG OFF\")\n",
    "                debugToggle=False\n",
    "                continue\n",
    "        \n",
    "            # Print input shape and type\n",
    "            # inputs = self.interpreter.get_input_details()\n",
    "            # print('\\n{} output(s):'.format(len(inputs)))\n",
    "            # for i in range(0, len(inputs)):\n",
    "            #     print('{} {}'.format(inputs[i]['shape'], inputs[i]['dtype'])) \n",
    "\n",
    "            # # Print output shape and type\n",
    "            # outputs = self.interpreter.get_output_details()\n",
    "            # print('\\n{} output(s):'.format(len(outputs)))\n",
    "            # for i in range(0, len(outputs)):\n",
    "            #     print('{} {}'.format(outputs[i]['shape'], outputs[i]['dtype']))   \n",
    "\n",
    "            intent,pos=self.preprocess(inp)\n",
    "\n",
    "            self.interpreter.set_tensor(self.input_details[1][\"index\"],pos)\n",
    "            self.interpreter.set_tensor(self.input_details[0][\"index\"],intent)\n",
    "            self.interpreter.invoke()\n",
    "            pos_result = self.interpreter.get_tensor(self.output_details[0]['index']) \n",
    "            intent_result = self.interpreter.get_tensor(self.output_details[1]['index'])\n",
    "            # rs=myModel.predict([intent,pos])\n",
    "            pos_rs=self.posAnalyze(myTokenizer.bi_gram_ize(inp),pos_result)\n",
    "            brain_dict=self.brain.knowledge_lookup(pos_rs)\n",
    "            intent_output=self.labellist[self.accept_accuracy(intent_result,0.8)]\n",
    "\n",
    "            print(\"You: \",inp)\n",
    "            if debugToggle==True:\n",
    "                self.debugMode([\n",
    "                    [\"POS: \",pos_rs],\n",
    "                    [\"Intent: \",intent_output],\n",
    "                    [\"Acc: \",max(intent_result[0])],\n",
    "                    [\"Bot: \",self.bot_action(intent_output,pos_rs,brain_dict)],\n",
    "                    [\"Brain Check: \",brain_dict]\n",
    "                    ])\n",
    "            else:\n",
    "                print(\"Bot: \",self.bot_action(intent_output,pos_rs,brain_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV: DEBUG ON\n",
      "You:  chào cậu\n",
      "0 / ['POS: ', [['chào', 'verb'], ['cậu', 'noun']]]\n",
      "1 / ['Intent: ', 'greeting']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'chào cậu']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Hôm nay sao rồi\n",
      "0 / ['POS: ', [['hôm nay', 'noun'], ['sao', 'part'], ['rồi', 'x']]]\n",
      "1 / ['Intent: ', '2ndgreeting']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Rất tốt']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Giúp mình làm hóa được không\n",
      "0 / ['POS: ', [['giúp', 'verb'], ['mình', 'propn'], ['làm', 'verb'], ['hóa', 'x'], ['được', 'x'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Mình biết vấn đề của bạn được không ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu giúp mình làm toán với\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['giúp', 'verb'], ['mình', 'propn'], ['làm', 'verb'], ['toán', 'noun'], ['với', 'adp']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Mình luôn ở đây mà']\n",
      "4 / ['Brain Check: ', [['toán', {'favorite': 'yes', 'info': ['tính toán', 'phương trình', 'đạo hàm'], 'define': 'Toán học là khoa học về các mô hình trừu tượng tổng quát, dùng để biểu diễn và phân tích mọi thứ trên thế giới.'}]]]\n",
      "You:  Bạn thích cái gì nhất\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['cái', 'noun'], ['gì', 'x'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.5208131]\n",
      "3 / ['Bot: ', 'Là sao vậy ? Mình không hiểu cậu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cách nào để học tốt hơn vậy\n",
      "0 / ['POS: ', [['cách', 'noun'], ['nào', 'propn'], ['để', 'adp'], ['học', 'verb'], ['tốt', 'adj'], ['hơn', 'x'], ['vậy', 'part']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Học tài thi phận\n",
      "0 / ['POS: ', [['học', 'verb'], ['tài', 'noun'], ['thi', 'verb'], ['phận', 'noun']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.5207669]\n",
      "3 / ['Bot: ', 'Là sao vậy ? Mình không hiểu cậu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Bạn thích the Queens không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['the', 'noun'], ['queens', 'verb'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.9999727]\n",
      "3 / ['Bot: ', 'Không']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  bạn thích the queen không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['the', 'noun'], ['queen', 'verb'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.9999727]\n",
      "3 / ['Bot: ', 'Không']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Bạn thích chơi minecraft không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['chơi', 'verb'], ['minecraft', 'noun'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.7761473]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  bạn thích làm gì đó không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['làm_gì', 'x'], ['đó', 'propn'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.5732034]\n",
      "3 / ['Bot: ', 'Là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  bạn thích làm gì khi rãnh\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['làm_gì', 'x'], ['khi', 'noun'], ['rãnh', 'noun']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9994783]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Bạn thích chơi game gì\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['chơi', 'verb'], ['game', 'noun'], ['gì', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9998344]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Cậu thích game gì nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['game', 'noun'], ['gì', 'propn'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.4433683]\n",
      "3 / ['Bot: ', 'Là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích môn học nào nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['môn học', 'noun'], ['nào', 'propn'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích ban nhạc gì nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['ban', 'noun'], ['nhạc', 'noun'], ['gì', 'propn'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.98224235]\n",
      "3 / ['Bot: ', 'Nah']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích ban gì nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['ban', 'noun'], ['gì', 'x'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.4433683]\n",
      "3 / ['Bot: ', 'Là sao vậy ? Mình không hiểu cậu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích thể loại nhạc nào\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['thể loại', 'noun'], ['nhạc', 'noun'], ['nào', 'propn']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 0.9508244]\n",
      "3 / ['Bot: ', 'Vấn đề của bạn là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích thể loại nhạc nào vậy\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['thể loại', 'noun'], ['nhạc', 'noun'], ['nào', 'propn'], ['vậy', 'part']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9999999]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích thể loại nhạc gì\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['thể loại', 'noun'], ['nhạc', 'noun'], ['gì', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9957587]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích gì nào\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['gì', 'propn'], ['nào', 'propn']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 0.95723563]\n",
      "3 / ['Bot: ', 'Mình biết vấn đề của bạn được không ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Môn học nào bạn thích nhất\n",
      "0 / ['POS: ', [['môn học', 'noun'], ['nào', 'propn'], ['bạn', 'noun'], ['thích', 'verb'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Học lập trình khó không\n",
      "0 / ['POS: ', [['học', 'verb'], ['lập trình', 'noun'], ['khó', 'adj'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.45425293]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tôi có việc cần nhờ bạn\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['có', 'verb'], ['việc', 'noun'], ['cần', 'verb'], ['nhờ', 'verb'], ['bạn', 'noun']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 0.8171846]\n",
      "3 / ['Bot: ', 'Vấn đề của bạn là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tôi có việc cần nhờ cậu\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['có', 'verb'], ['việc', 'noun'], ['cần', 'verb'], ['nhờ', 'verb'], ['cậu', 'noun']]]\n",
      "1 / ['Intent: ', 'greeting']\n",
      "2 / ['Acc: ', 0.93832695]\n",
      "3 / ['Bot: ', 'Hello!']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Tôi có việc gấp rồi\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['có', 'verb'], ['việc', 'noun'], ['gấp', 'adj'], ['rồi', 'x']]]\n",
      "1 / ['Intent: ', '2ndgreeting']\n",
      "2 / ['Acc: ', 0.9931606]\n",
      "3 / ['Bot: ', 'Rất tốt']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  môn hóa là gì\n",
      "0 / ['POS: ', [['môn', 'noun'], ['hóa', 'x'], ['là', 'cconj'], ['gì', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.99999905]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Cậu biết gì không\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['biết', 'verb'], ['gì', 'propn'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.9999944]\n",
      "3 / ['Bot: ', 'Nah']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  này bạn\n",
      "0 / ['POS: ', [['này', 'propn'], ['bạn', 'noun']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.77242893]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Tớ không biết giải bài này\n",
      "0 / ['POS: ', [['tớ', 'noun'], ['không', 'x'], ['biết', 'verb'], ['giải', 'noun'], ['bài', 'noun'], ['này', 'propn']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Nah']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Tôi không biết giải bài này\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['không', 'x'], ['biết', 'verb'], ['giải', 'noun'], ['bài', 'noun'], ['này', 'propn']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'No']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tôi ghét Minecraft\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['ghét', 'verb'], ['minecraft', 'verb']]]\n",
      "1 / ['Intent: ', 'goodbye']\n",
      "2 / ['Acc: ', 0.8307155]\n",
      "3 / ['Bot: ', 'Sad to see you go :(']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Làm toán không\n",
      "0 / ['POS: ', [['làm', 'verb'], ['toán', 'noun'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.4111723]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', [['toán', {'favorite': 'yes', 'info': ['tính toán', 'phương trình', 'đạo hàm'], 'define': 'Toán học là khoa học về các mô hình trừu tượng tổng quát, dùng để biểu diễn và phân tích mọi thứ trên thế giới.'}]]]\n",
      "You:  Ăn pizza không\n",
      "0 / ['POS: ', [['ăn', 'verb'], ['pizza', 'noun'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.70244354]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', [['pizza', {'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'}]]]\n",
      "You:  Cách nào để mình có thể tốt hơn nhỉ\n",
      "0 / ['POS: ', [['cách', 'noun'], ['nào', 'propn'], ['để', 'adp'], ['mình', 'propn'], ['có_thể', 'x'], ['tốt', 'adj'], ['hơn', 'x'], ['nhỉ', 'intj']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cách nào\n",
      "0 / ['POS: ', [['cách', 'noun'], ['nào', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tốt hơn\n",
      "0 / ['POS: ', [['tốt', 'adj'], ['hơn', 'x']]]\n",
      "1 / ['Intent: ', 'user_amazed']\n",
      "2 / ['Acc: ', 0.9999281]\n",
      "3 / ['Bot: ', 'Yip']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  có thể\n",
      "0 / ['POS: ', [['có_thể', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.49231526]\n",
      "3 / ['Bot: ', 'Là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n"
     ]
    }
   ],
   "source": [
    "botBackend=BotBackendLite(mpos,data,bi_gram_list,labellist)\n",
    "botBackend.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44a47853b1918eb710e25fd6ba6ee9af6fac694ece621a4756b81ff23436e8c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
