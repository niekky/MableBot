{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/intentsVNver1.json\",encoding=\"utf-8\") as file:\r\n",
    "    data=json.load(file)\r\n",
    "patternlist=[]\r\n",
    "labels=[]\r\n",
    "for dt in data[\"intents\"]:\r\n",
    "    for pattern in dt[\"patterns\"]:\r\n",
    "        patternlist.append(pattern)\r\n",
    "        labels.append(dt[\"tag\"])\r\n",
    "labels\r\n",
    "labels=np.array(labels)\r\n",
    "labelsOH=pd.get_dummies(labels)\r\n",
    "labellist=list(labelsOH)\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/bi_grams.txt\",encoding=\"utf-8\") as f:\r\n",
    "    bi_gramtxt=f.read()\r\n",
    "bi_gram_list=[]\r\n",
    "for each in bi_gramtxt.split(\",\"):\r\n",
    "    each=each[:-1]\r\n",
    "    each=each[2:]\r\n",
    "    each=each.replace(\" \",\"_\")\r\n",
    "    bi_gram_list.append(each)\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/vietnamese-stopwords.txt\",encoding=\"utf-8\") as f:\r\n",
    "    stopword_txt=f.readlines()\r\n",
    "    \r\n",
    "stopword_list=[]\r\n",
    "for each in stopword_txt:\r\n",
    "    stopword_list.append(each[:-1])\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/posdata.json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "    mpos=json.load(f)\r\n",
    "\r\n",
    "responses_dict={}\r\n",
    "for i in data[\"intents\"]:\r\n",
    "    responses_dict[i[\"tag\"]]=i[\"responses\"]\r\n",
    "\r\n",
    "with open(\"E:/MLCourse/foxBot/foxPython/End2EndModel/data/knowledge.json\",\"r\",encoding=\"utf-8\") as f:\r\n",
    "    knowledge=json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "list(knowledge.values())[0][\"pizza\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "list(knowledge.values())[0]\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'pizza': {'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'},\n",
       " 'phở': {'favorite': 'yes',\n",
       "  'info': ['ngon', 'ngon hơn nữa'],\n",
       "  'xuất xứ': 'Việt Nam'},\n",
       " 'đậu xanh': {'favorite': 'no', 'info': ['không ngon']}}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class brain_exe():\r\n",
    "    def __init__(self,knowledge):\r\n",
    "        self.mem={}\r\n",
    "        self.dialogue_count=0\r\n",
    "        self.length_limit=20\r\n",
    "        self.knowledge_base=knowledge\r\n",
    "\r\n",
    "    def pos_filter(self,pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\r\n",
    "        pos_filtered=[]\r\n",
    "        for i in pos_rs:\r\n",
    "            if (i[1]==pos_name and stopword.count(i[0])==0):\r\n",
    "                pos_filtered.append(i)\r\n",
    "        return pos_filtered\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.mem={}\r\n",
    "    \r\n",
    "    def add(self,intent,sentence_entities):\r\n",
    "        self.mem[self.dialogue_count]=[intent,sentence_entities]\r\n",
    "        self.dialogue_count=self.dialogue_count+1\r\n",
    "        if self.dialogue_count==self.length_limit:\r\n",
    "            self.mem={}\r\n",
    "\r\n",
    "    def lookup(self):\r\n",
    "        return self.mem\r\n",
    "\r\n",
    "    def knowledge_lookup(self,entities):\r\n",
    "        if len(entities)!=0:\r\n",
    "            verb_list=self.pos_filter(entities,\"verb\")\r\n",
    "            noun_list=self.pos_filter(entities,\"noun\")\r\n",
    "            result=[]\r\n",
    "            for i in noun_list:\r\n",
    "                i[0]=i[0].replace(\"_\",\" \")\r\n",
    "                for each in list(knowledge.values()):\r\n",
    "                    for j in each.keys():\r\n",
    "                        if i[0]==j:\r\n",
    "                            result.append([j,each[j]])\r\n",
    "            return result\r\n",
    "        else:\r\n",
    "            return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class FTokenizer():\r\n",
    "    def __init__(self,bi_gram_list):\r\n",
    "        self.bi_gram_list=bi_gram_list\r\n",
    "\r\n",
    "    def bi_gram_checker(self,bi_gram_word):\r\n",
    "        for i in self.bi_gram_list:\r\n",
    "            if (bi_gram_word==i):\r\n",
    "                return True\r\n",
    "        return False\r\n",
    "    \r\n",
    "    def fit_on_text(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (myContinue):\r\n",
    "                    myContinue=False\r\n",
    "                    continue\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        myContinue=True\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (myContinue==False):  \r\n",
    "                    if (tokens.count(listtokens[i])==0):\r\n",
    "                        tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "\r\n",
    "    def fit_on_text2(self,data):\r\n",
    "        word_to_index={}\r\n",
    "        index_to_word={}\r\n",
    "        word_to_index[\"<PAD>\"]=0\r\n",
    "        word_to_index[\"<OOV>\"]=1\r\n",
    "        index_to_word[0]=\"<PAD>\"\r\n",
    "        index_to_word[1]=\"<OOV>\"\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]\r\n",
    "        for eachseq in data:\r\n",
    "            listtokens=eachseq.lower().split(\" \")\r\n",
    "            listtokens.append(\"\")\r\n",
    "            for i in range(0,len(listtokens)-1):\r\n",
    "                if (listtokens[i+1]!=\"\"):\r\n",
    "                    pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                    if (self.bi_gram_checker(pairtext)):\r\n",
    "                        if (tokens.count(pairtext)==0):\r\n",
    "                            tokens.append(pairtext)\r\n",
    "                if (tokens.count(listtokens[i])==0 and listtokens[i]!=\"\"):\r\n",
    "                    tokens.append(listtokens[i])\r\n",
    "        for index,word in enumerate(tokens):\r\n",
    "            word_to_index[word]=index+2\r\n",
    "            index_to_word[index+2]=word\r\n",
    "        self.word_to_index=word_to_index\r\n",
    "        self.index_to_word=index_to_word\r\n",
    "    \r\n",
    "    def get_word_to_index(self):\r\n",
    "        return self.word_to_index\r\n",
    "    \r\n",
    "    def get_index_to_word(self):\r\n",
    "        return self.index_to_word\r\n",
    "\r\n",
    "    def get_max_length(self,data):\r\n",
    "        maxsen=0\r\n",
    "        for i in data:\r\n",
    "            if maxsen<len(i):\r\n",
    "                maxsen=len(i)\r\n",
    "        return maxsen\r\n",
    "\r\n",
    "    def text_to_sequence(self,textdata):\r\n",
    "        d_continue=False\r\n",
    "        output=[]\r\n",
    "        for text in textdata:\r\n",
    "            text=text.lower().split(\" \")\r\n",
    "            text.append(\"\")\r\n",
    "            sth=[]\r\n",
    "            d_continue=False\r\n",
    "            for i in range(0,len(text)-1):\r\n",
    "                if d_continue==True:\r\n",
    "                    d_continue=False\r\n",
    "                    continue\r\n",
    "                if (text[i+1]!=\"\"):\r\n",
    "                    pairtext=text[i]+\"_\"+text[i+1]\r\n",
    "                    for j in self.word_to_index:\r\n",
    "                        if pairtext.lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            d_continue=True\r\n",
    "                            break\r\n",
    "                if d_continue==False:\r\n",
    "                    for count,j in enumerate(self.word_to_index):\r\n",
    "                        if text[i].lower()==j:\r\n",
    "                            sth.append(self.word_to_index[j])\r\n",
    "                            break\r\n",
    "                        if count==len(self.word_to_index)-1:\r\n",
    "                            sth.append(1)\r\n",
    "            output.append(sth)\r\n",
    "        return output\r\n",
    "    \r\n",
    "    def intseq_to_index(self,sequence):\r\n",
    "        output=[]\r\n",
    "        for i in sequence:\r\n",
    "            output.append(self.index_to_word[i])\r\n",
    "        return output\r\n",
    "\r\n",
    "    def pad_sequence(self,datasequence,maxlen):\r\n",
    "        for i in datasequence:\r\n",
    "            while len(i)!=maxlen:\r\n",
    "                i.insert(0,0)\r\n",
    "        return datasequence\r\n",
    "\r\n",
    "    def bi_gram_ize(self,text):\r\n",
    "        listtokens=text.lower().split(\" \")\r\n",
    "        listtokens.append(\"\")\r\n",
    "        myContinue=False\r\n",
    "        tokens=[]   \r\n",
    "        for i in range(0,len(listtokens)-1):\r\n",
    "            if (myContinue):\r\n",
    "                myContinue=False\r\n",
    "                continue\r\n",
    "            if (listtokens[i+1]!=\"\"):\r\n",
    "                pairtext=listtokens[i]+\"_\"+listtokens[i+1]\r\n",
    "                if (self.bi_gram_checker(pairtext)):\r\n",
    "                    myContinue=True\r\n",
    "                    tokens.append(pairtext)\r\n",
    "            if (myContinue==False):  \r\n",
    "                tokens.append(listtokens[i])\r\n",
    "        return tokens\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "myTokenizer=FTokenizer(bi_gram_list)\r\n",
    "myTokenizer.fit_on_text2(patternlist)\r\n",
    "pattern_tokenized2=myTokenizer.text_to_sequence(patternlist)\r\n",
    "pattern_preprocessed2=tf.keras.preprocessing.sequence.pad_sequences(pattern_tokenized2)\r\n",
    "mlen2=pattern_preprocessed2.shape[1]\r\n",
    "\r\n",
    "posmesTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token=\"<UNK>\")\r\n",
    "posmesTokenizer.fit_on_texts(mpos[\"messages\"])\r\n",
    "mes_Tokenized=posmesTokenizer.texts_to_sequences(mpos[\"messages\"])\r\n",
    "finalmes=tf.keras.preprocessing.sequence.pad_sequences(mes_Tokenized)\r\n",
    "poslen=finalmes.shape[1]\r\n",
    "\r\n",
    "postagTokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=20000)\r\n",
    "postagTokenizer.fit_on_texts(mpos[\"pos\"])\r\n",
    "pos_tokenized=postagTokenizer.texts_to_sequences(mpos[\"pos\"])\r\n",
    "finalpos=tf.keras.preprocessing.sequence.pad_sequences(pos_tokenized)\r\n",
    "postagTokenizer.index_word[0]=\"<PAD>\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "intentdict=myTokenizer.word_to_index\r\n",
    "posmesdict=posmesTokenizer.word_index\r\n",
    "postagdict=postagTokenizer.word_index\r\n",
    "this_bi_gram_lict=bi_gram_list\r\n",
    "posmesdict[\"<PAD>\"]=0\r\n",
    "postagdict[\"<PAD>\"]=0\r\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/intentDict.json\",\"w\",encoding=\"utf-8\") as f:\r\n",
    "    json.dump(intentdict,f)\r\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/posmesDict.json\",\"w\",encoding=\"utf-8\") as f:\r\n",
    "    json.dump(posmesdict,f)\r\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/postagDict.json\",\"w\",encoding=\"utf-8\") as f:\r\n",
    "    json.dump(postagdict,f)\r\n",
    "with open(\"E:/MLCourse/TF2/Notebook/tutorialNotebook/data/word_dictionary/this_bg_list.json\",\"w\",encoding=\"utf-8\") as f:\r\n",
    "    json.dump(this_bi_gram_lict,f)  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class BotBackendLite():\r\n",
    "    def __init__(self,posdata,intentdata,bi_gram_list,labellist):\r\n",
    "        self.posdata=posdata\r\n",
    "        self.intent_data=intentdata\r\n",
    "        self.bi_gram_list=bi_gram_list\r\n",
    "        self.labellist=np.array(labellist+[\"NULL\"])\r\n",
    "        self.response_dictionary=responses_dict\r\n",
    "        self.response_dictionary[\"NULL\"]=[\"Mình không hiểu lắm\",\"Là sao vậy ? Mình không hiểu cậu lắm\",\"Là gì thế ?\"]\r\n",
    "\r\n",
    "        self.interpreter=tf.lite.Interpreter(\"E:/MLCourse/foxBot/foxPython/End2EndModel/model/tfliteModel/model3.tflite\")\r\n",
    "        self.interpreter.allocate_tensors()\r\n",
    "        self.input_details=self.interpreter.get_input_details()\r\n",
    "        self.output_details=self.interpreter.get_output_details()\r\n",
    "        self.input_shape1=self.input_details[0][\"shape\"]\r\n",
    "        self.input_shape2=self.input_details[1][\"shape\"]\r\n",
    "        self.brain=brain_exe(knowledge)\r\n",
    "\r\n",
    "    def preprocess(self,inp):\r\n",
    "        punc_list=[\",\",\".\",\"'\",\"[\",\"]\",\"!\",\"?\"]\r\n",
    "        for i in punc_list:\r\n",
    "            inp=inp.replace(i,\"\")\r\n",
    "        inp_tokenized=myTokenizer.text_to_sequence([inp])\r\n",
    "        inp_preprocessed=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=mlen2)\r\n",
    "\r\n",
    "        inp_gramized=myTokenizer.bi_gram_ize(inp)\r\n",
    "        inp_tokenized=posmesTokenizer.texts_to_sequences([inp_gramized])\r\n",
    "        final_inp=tf.keras.preprocessing.sequence.pad_sequences(inp_tokenized,maxlen=finalmes.shape[1])\r\n",
    "        return inp_preprocessed,final_inp\r\n",
    "\r\n",
    "    def posAnalyze(self,inp,posdata):\r\n",
    "        pos=[]\r\n",
    "        for i in posdata[0]:\r\n",
    "            if postagTokenizer.index_word[np.argmax(i)]!=\"<PAD>\":\r\n",
    "                pos.append(postagTokenizer.index_word[np.argmax(i)])\r\n",
    "        l=[]\r\n",
    "        for i in range(0,len(pos)):\r\n",
    "            if pos[i]!=\"<PAD>\":\r\n",
    "                l.append([inp[i],pos[i]])\r\n",
    "        return l\r\n",
    "\r\n",
    "    def pos_filter(self,pos_rs,pos_name,stopword=[\"bạn\",\"tớ\",\"cậu\",\"tí\",\"này\",\"lát\",\"chút\",\"nhé\",\"nha\",\"mình\"]):\r\n",
    "        pos_filtered=[]\r\n",
    "        for i in pos_rs:\r\n",
    "            if (i[1]==pos_name and stopword.count(i[0])==0):\r\n",
    "                pos_filtered.append(i)\r\n",
    "        return pos_filtered\r\n",
    "\r\n",
    "    def accept_accuracy(self,pred,acc):\r\n",
    "        if (max(pred[0])>acc):\r\n",
    "            return np.argmax(pred[0])\r\n",
    "        else:\r\n",
    "            return len(pred[0])\r\n",
    "\r\n",
    "    def debugMode(self,printlist):\r\n",
    "        for count,i in enumerate(printlist):\r\n",
    "            print(count,\"/\",i)\r\n",
    "\r\n",
    "    def responses_generate(self,inp):\r\n",
    "        return random.choice(self.response_dictionary[inp])\r\n",
    "\r\n",
    "    def bot_action(self,inp,entities,brain_dict=False):\r\n",
    "        if inp==\"question_from_user\":\r\n",
    "            if len(brain_dict)!=0:\r\n",
    "                try:\r\n",
    "                    return brain_dict[0][1][\"define\"]\r\n",
    "                except:\r\n",
    "                    return \"Nó khó tả lắm sorry\"\r\n",
    "            return \"Sorry, hiện tại mình chưa biết nữa\"\r\n",
    "\r\n",
    "        if inp==\"askingme\":\r\n",
    "            verb_list=self.pos_filter(entities,\"verb\")\r\n",
    "            noun_list=self.pos_filter(entities,\"noun\")\r\n",
    "            if len(brain_dict)!=0:\r\n",
    "                return random.choice([\"Có\",\"Yess\",\"Yes\"])\r\n",
    "            else:\r\n",
    "                return random.choice([\"Không\",\"No\",\"Nah\"])\r\n",
    "        \r\n",
    "        if inp!=\"askingme\" and inp!=\"question_from_user\":\r\n",
    "            return self.responses_generate(inp)\r\n",
    "  \r\n",
    "    def chat(self):\r\n",
    "        debugToggle=False\r\n",
    "        while True:\r\n",
    "            inp=input(\"You: \")\r\n",
    "            # Hotkey\r\n",
    "            if inp==\"quit\":\r\n",
    "                break\r\n",
    "            if inp==\"!debugmode1\":\r\n",
    "                print(\"DEV: DEBUG ON\")\r\n",
    "                debugToggle=True\r\n",
    "                continue\r\n",
    "            if inp==\"!debugmode0\":\r\n",
    "                print(\"DEV: DEBUG OFF\")\r\n",
    "                debugToggle=False\r\n",
    "                continue\r\n",
    "        \r\n",
    "            # Print input shape and type\r\n",
    "            # inputs = self.interpreter.get_input_details()\r\n",
    "            # print('\\n{} output(s):'.format(len(inputs)))\r\n",
    "            # for i in range(0, len(inputs)):\r\n",
    "            #     print('{} {}'.format(inputs[i]['shape'], inputs[i]['dtype'])) \r\n",
    "\r\n",
    "            # # Print output shape and type\r\n",
    "            # outputs = self.interpreter.get_output_details()\r\n",
    "            # print('\\n{} output(s):'.format(len(outputs)))\r\n",
    "            # for i in range(0, len(outputs)):\r\n",
    "            #     print('{} {}'.format(outputs[i]['shape'], outputs[i]['dtype']))   \r\n",
    "\r\n",
    "            intent,pos=self.preprocess(inp)\r\n",
    "\r\n",
    "            self.interpreter.set_tensor(self.input_details[1][\"index\"],pos)\r\n",
    "            self.interpreter.set_tensor(self.input_details[0][\"index\"],intent)\r\n",
    "            self.interpreter.invoke()\r\n",
    "            pos_result = self.interpreter.get_tensor(self.output_details[0]['index']) \r\n",
    "            intent_result = self.interpreter.get_tensor(self.output_details[1]['index'])\r\n",
    "            # rs=myModel.predict([intent,pos])\r\n",
    "            pos_rs=self.posAnalyze(myTokenizer.bi_gram_ize(inp),pos_result)\r\n",
    "            brain_dict=self.brain.knowledge_lookup(pos_rs)\r\n",
    "            intent_output=self.labellist[self.accept_accuracy(intent_result,0.8)]\r\n",
    "\r\n",
    "            print(\"You: \",inp)\r\n",
    "            if debugToggle==True:\r\n",
    "                self.debugMode([\r\n",
    "                    [\"POS: \",pos_rs],\r\n",
    "                    [\"Intent: \",intent_output],\r\n",
    "                    [\"Acc: \",max(intent_result[0])],\r\n",
    "                    [\"Bot: \",self.bot_action(intent_output,pos_rs,brain_dict)],\r\n",
    "                    [\"Brain Check: \",brain_dict]\r\n",
    "                    ])\r\n",
    "            else:\r\n",
    "                print(\"Bot: \",self.bot_action(intent_output,pos_rs,brain_dict))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "botBackend=BotBackendLite(mpos,data,bi_gram_list,labellist)\r\n",
    "botBackend.chat()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEV: DEBUG ON\n",
      "You:  chào cậu\n",
      "0 / ['POS: ', [['chào', 'verb'], ['cậu', 'noun']]]\n",
      "1 / ['Intent: ', 'greeting']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'chào cậu']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Hôm nay sao rồi\n",
      "0 / ['POS: ', [['hôm nay', 'noun'], ['sao', 'part'], ['rồi', 'x']]]\n",
      "1 / ['Intent: ', '2ndgreeting']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Rất tốt']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Giúp mình làm hóa được không\n",
      "0 / ['POS: ', [['giúp', 'verb'], ['mình', 'propn'], ['làm', 'verb'], ['hóa', 'x'], ['được', 'x'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Mình biết vấn đề của bạn được không ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu giúp mình làm toán với\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['giúp', 'verb'], ['mình', 'propn'], ['làm', 'verb'], ['toán', 'noun'], ['với', 'adp']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Mình luôn ở đây mà']\n",
      "4 / ['Brain Check: ', [['toán', {'favorite': 'yes', 'info': ['tính toán', 'phương trình', 'đạo hàm'], 'define': 'Toán học là khoa học về các mô hình trừu tượng tổng quát, dùng để biểu diễn và phân tích mọi thứ trên thế giới.'}]]]\n",
      "You:  Bạn thích cái gì nhất\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['cái', 'noun'], ['gì', 'x'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.5208131]\n",
      "3 / ['Bot: ', 'Là sao vậy ? Mình không hiểu cậu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cách nào để học tốt hơn vậy\n",
      "0 / ['POS: ', [['cách', 'noun'], ['nào', 'propn'], ['để', 'adp'], ['học', 'verb'], ['tốt', 'adj'], ['hơn', 'x'], ['vậy', 'part']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Học tài thi phận\n",
      "0 / ['POS: ', [['học', 'verb'], ['tài', 'noun'], ['thi', 'verb'], ['phận', 'noun']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.5207669]\n",
      "3 / ['Bot: ', 'Là sao vậy ? Mình không hiểu cậu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Bạn thích the Queens không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['the', 'noun'], ['queens', 'verb'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.9999727]\n",
      "3 / ['Bot: ', 'Không']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  bạn thích the queen không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['the', 'noun'], ['queen', 'verb'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.9999727]\n",
      "3 / ['Bot: ', 'Không']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Bạn thích chơi minecraft không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['chơi', 'verb'], ['minecraft', 'noun'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.7761473]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  bạn thích làm gì đó không\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['làm_gì', 'x'], ['đó', 'propn'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.5732034]\n",
      "3 / ['Bot: ', 'Là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  bạn thích làm gì khi rãnh\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['làm_gì', 'x'], ['khi', 'noun'], ['rãnh', 'noun']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9994783]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Bạn thích chơi game gì\n",
      "0 / ['POS: ', [['bạn', 'noun'], ['thích', 'verb'], ['chơi', 'verb'], ['game', 'noun'], ['gì', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9998344]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Cậu thích game gì nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['game', 'noun'], ['gì', 'propn'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.4433683]\n",
      "3 / ['Bot: ', 'Là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích môn học nào nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['môn học', 'noun'], ['nào', 'propn'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích ban nhạc gì nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['ban', 'noun'], ['nhạc', 'noun'], ['gì', 'propn'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.98224235]\n",
      "3 / ['Bot: ', 'Nah']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích ban gì nhất\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['ban', 'noun'], ['gì', 'x'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.4433683]\n",
      "3 / ['Bot: ', 'Là sao vậy ? Mình không hiểu cậu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích thể loại nhạc nào\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['thể loại', 'noun'], ['nhạc', 'noun'], ['nào', 'propn']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 0.9508244]\n",
      "3 / ['Bot: ', 'Vấn đề của bạn là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích thể loại nhạc nào vậy\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['thể loại', 'noun'], ['nhạc', 'noun'], ['nào', 'propn'], ['vậy', 'part']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9999999]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích thể loại nhạc gì\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['thể loại', 'noun'], ['nhạc', 'noun'], ['gì', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.9957587]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cậu thích gì nào\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['thích', 'verb'], ['gì', 'propn'], ['nào', 'propn']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 0.95723563]\n",
      "3 / ['Bot: ', 'Mình biết vấn đề của bạn được không ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Môn học nào bạn thích nhất\n",
      "0 / ['POS: ', [['môn học', 'noun'], ['nào', 'propn'], ['bạn', 'noun'], ['thích', 'verb'], ['nhất', 'x']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Học lập trình khó không\n",
      "0 / ['POS: ', [['học', 'verb'], ['lập trình', 'noun'], ['khó', 'adj'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.45425293]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tôi có việc cần nhờ bạn\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['có', 'verb'], ['việc', 'noun'], ['cần', 'verb'], ['nhờ', 'verb'], ['bạn', 'noun']]]\n",
      "1 / ['Intent: ', 'Assist']\n",
      "2 / ['Acc: ', 0.8171846]\n",
      "3 / ['Bot: ', 'Vấn đề của bạn là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tôi có việc cần nhờ cậu\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['có', 'verb'], ['việc', 'noun'], ['cần', 'verb'], ['nhờ', 'verb'], ['cậu', 'noun']]]\n",
      "1 / ['Intent: ', 'greeting']\n",
      "2 / ['Acc: ', 0.93832695]\n",
      "3 / ['Bot: ', 'Hello!']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Tôi có việc gấp rồi\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['có', 'verb'], ['việc', 'noun'], ['gấp', 'adj'], ['rồi', 'x']]]\n",
      "1 / ['Intent: ', '2ndgreeting']\n",
      "2 / ['Acc: ', 0.9931606]\n",
      "3 / ['Bot: ', 'Rất tốt']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  môn hóa là gì\n",
      "0 / ['POS: ', [['môn', 'noun'], ['hóa', 'x'], ['là', 'cconj'], ['gì', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 0.99999905]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Cậu biết gì không\n",
      "0 / ['POS: ', [['cậu', 'noun'], ['biết', 'verb'], ['gì', 'propn'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 0.9999944]\n",
      "3 / ['Bot: ', 'Nah']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  này bạn\n",
      "0 / ['POS: ', [['này', 'propn'], ['bạn', 'noun']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.77242893]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Tớ không biết giải bài này\n",
      "0 / ['POS: ', [['tớ', 'noun'], ['không', 'x'], ['biết', 'verb'], ['giải', 'noun'], ['bài', 'noun'], ['này', 'propn']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Nah']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Tôi không biết giải bài này\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['không', 'x'], ['biết', 'verb'], ['giải', 'noun'], ['bài', 'noun'], ['này', 'propn']]]\n",
      "1 / ['Intent: ', 'askingme']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'No']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tôi ghét Minecraft\n",
      "0 / ['POS: ', [['tôi', 'propn'], ['ghét', 'verb'], ['minecraft', 'verb']]]\n",
      "1 / ['Intent: ', 'goodbye']\n",
      "2 / ['Acc: ', 0.8307155]\n",
      "3 / ['Bot: ', 'Sad to see you go :(']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  Làm toán không\n",
      "0 / ['POS: ', [['làm', 'verb'], ['toán', 'noun'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.4111723]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', [['toán', {'favorite': 'yes', 'info': ['tính toán', 'phương trình', 'đạo hàm'], 'define': 'Toán học là khoa học về các mô hình trừu tượng tổng quát, dùng để biểu diễn và phân tích mọi thứ trên thế giới.'}]]]\n",
      "You:  Ăn pizza không\n",
      "0 / ['POS: ', [['ăn', 'verb'], ['pizza', 'noun'], ['không', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.70244354]\n",
      "3 / ['Bot: ', 'Mình không hiểu lắm']\n",
      "4 / ['Brain Check: ', [['pizza', {'favorite': 'yes', 'info': ['ngon', 'rất ngon'], 'xuất xứ': 'Ý'}]]]\n",
      "You:  Cách nào để mình có thể tốt hơn nhỉ\n",
      "0 / ['POS: ', [['cách', 'noun'], ['nào', 'propn'], ['để', 'adp'], ['mình', 'propn'], ['có_thể', 'x'], ['tốt', 'adj'], ['hơn', 'x'], ['nhỉ', 'intj']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  cách nào\n",
      "0 / ['POS: ', [['cách', 'noun'], ['nào', 'propn']]]\n",
      "1 / ['Intent: ', 'question_from_user']\n",
      "2 / ['Acc: ', 1.0]\n",
      "3 / ['Bot: ', 'Sorry, hiện tại mình chưa biết nữa']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  tốt hơn\n",
      "0 / ['POS: ', [['tốt', 'adj'], ['hơn', 'x']]]\n",
      "1 / ['Intent: ', 'user_amazed']\n",
      "2 / ['Acc: ', 0.9999281]\n",
      "3 / ['Bot: ', 'Yip']\n",
      "4 / ['Brain Check: ', []]\n",
      "You:  có thể\n",
      "0 / ['POS: ', [['có_thể', 'x']]]\n",
      "1 / ['Intent: ', 'NULL']\n",
      "2 / ['Acc: ', 0.49231526]\n",
      "3 / ['Bot: ', 'Là gì thế ?']\n",
      "4 / ['Brain Check: ', []]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "3d95097792c4c1f8621d6eabaad3727eb6f403675c9e5393220cb43082419a68"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}